{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries included in Python distribution\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Installed libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import GifImagePlugin\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Navigate through all images in directory iteratively and display in pandas dataframe the dimensions of the images\n",
    "\n",
    "**Methodology** :\n",
    "1. Use *PIL* package to convert the gif to numpy.\n",
    "2. Obtain full path to file and then the shape of the image.\n",
    "\n",
    "**Objectives** : \n",
    "1. Find out if all the images are of the same dimension.\n",
    "2. Store information of all image files to be able to traverse easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path where files will be stored.\n",
    "# This is unpacked from the pickle file created in Step 0.\n",
    "\n",
    "with open('pickledHomeScratchShared.pickle', \"rb\") as f:\n",
    "    baseHomePath,baseScratchPath,baseSharedPath = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transverse - Navigate through the whole directory and build all paths if 'anon_111_t88_gfc{}tra_90' files are found\n",
    "\n",
    "fileShapeList = list()\n",
    "for root, dirs, files in os.walk(\"{}/data\".format(baseScratchPath)):\n",
    "    for file in files:\n",
    "        if re.match(\"^.+anon_111_t88_gfc.+tra_90\\.gif$\",file):\n",
    "            fullFilePath = root+'/'+file\n",
    "            # Step 2: Convert the image to Numpy ndarray object and find the shape of the object\n",
    "            img = asarray(Image.open(fullFilePath))\n",
    "            mriImgID = re.search(r'/([^/]+)$', os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(fullFilePath))))).group(1)\n",
    "            fileShapeList.append((mriImgID,file,fullFilePath,img.shape)) #Delete later\n",
    "\n",
    "# Step 3: Display in form of pandas dataframe that contains all sessions from each MRI visit for all patients\n",
    "imageInfoDf_t = pd.DataFrame(fileShapeList,columns=['MRI_ID','File name','Full path','Shape of image'])\n",
    "imageInfoDf_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Coronal - Navigate through the whole directory and build all paths if 'anon_111_t88_gfc{}cor_110' files are found\n",
    "\n",
    "fileShapeList = list()\n",
    "for root, dirs, files in os.walk(\"{}/data\".format(baseScratchPath)):\n",
    "    for file in files:\n",
    "        if re.match(\"^.+anon_111_t88_gfc.+cor_110\\.gif$\",file):\n",
    "            fullFilePath = root+'/'+file\n",
    "            # Step 2: Convert the image to Numpy ndarray object and find the shape of the object\n",
    "            img = asarray(Image.open(fullFilePath))\n",
    "            mriImgID = re.search(r'/([^/]+)$', os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(fullFilePath))))).group(1)\n",
    "            fileShapeList.append((mriImgID,file,fullFilePath,img.shape)) #Delete later\n",
    "\n",
    "# Step 3: Display in form of pandas dataframe that contains all sessions from each MRI visit for all patients\n",
    "imageInfoDf_c = pd.DataFrame(fileShapeList,columns=['MRI_ID','File name','Full path','Shape of image'])\n",
    "imageInfoDf_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sagittal - Navigate through the whole directory and build all paths if 'anon_111_t88_gfc{}sag_95' files are found\n",
    "\n",
    "fileShapeList = list()\n",
    "for root, dirs, files in os.walk(\"{}/data\".format(baseScratchPath)):\n",
    "    for file in files:\n",
    "        if re.match(\"^.+anon_111_t88_gfc.+sag_95\\.gif$\",file):\n",
    "            fullFilePath = root+'/'+file\n",
    "            # Step 2: Convert the image to Numpy ndarray object and find the shape of the object\n",
    "            img = asarray(Image.open(fullFilePath))\n",
    "            mriImgID = re.search(r'/([^/]+)$', os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(fullFilePath))))).group(1)\n",
    "            fileShapeList.append((mriImgID,file,fullFilePath,img.shape)) #Delete later\n",
    "\n",
    "# Step 3: Display in form of pandas dataframe that contains all sessions from each MRI visit for all patients\n",
    "imageInfoDf_s = pd.DataFrame(fileShapeList,columns=['MRI_ID','File name','Full path','Shape of image'])\n",
    "imageInfoDf_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 : Load image using PIL.Image and display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to take absolute path to produce numpy\n",
    "def imgPathtoNp(path):\n",
    "    # Load using PIL.Image\n",
    "    img_arr = asarray(Image.open(path))\n",
    "    # Normalize the data\n",
    "    #img_arr = np.round((img_arr/img_arr.max())*255).astype(np.uint8)\n",
    "    return img_arr\n",
    "\n",
    "# Function to show image using Numpy ndarray\n",
    "def showImg(ndarr):\n",
    "    return plt.imshow(ndarr, cmap=plt.cm.gray_r, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random file from imageInfoDf and load using Nibabel\n",
    "randImgIndex = random.randint(0,len(imageInfoDf_c)-1)\n",
    "#sampleImg = nib.load(imageInfoDf.iloc[randImgIndex]['Full path'])\n",
    "\n",
    "sampleImgData = imgPathtoNp(imageInfoDf_c.iloc[250]['Full path'])\n",
    "print('Sample picked out is : {}'.format(imageInfoDf_c.iloc[randImgIndex]['Full path']))\n",
    "\n",
    "# Review shape of the Numpy ndarray that encodes the image data\n",
    "sampleImgData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static image showing data\n",
    "plt.close();\n",
    "\n",
    "# Code to display image\n",
    "showImg(sampleImgData);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 : Pull in CDR labels, transform to binary and then join to `ImageInfoDf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files containing labels\n",
    "oasis_1_datatable = pd.read_csv('{}/milestone_II_project/data/oasis_labelled_data/oasis_1_labelled_data.csv'.format(baseHomePath))\n",
    "oasis_2_datatable = pd.read_excel('{}/milestone_II_project/data/oasis_labelled_data/oasis_2_labelled_data.xlsx'.format(baseHomePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize content from both datatables and make it into a single style\n",
    "\n",
    "# Filter and transform to datasets that will be used in the project\n",
    "oasis1DataSet = oasis_1_datatable.copy() # 436 rows expected from this operation\n",
    "oasis1DataSet['CDR'] = oasis1DataSet.CDR.fillna(0) # Big assumption that 'NA' == not demented\n",
    "oasis1DataSet['dem_labels'] = oasis1DataSet['CDR'].map(lambda x: 0 if x==0 else 1)\n",
    "oasis1DataSet.rename(columns={'ID':'MRI_ID'},inplace=True)\n",
    "oasis1DataSet = oasis1DataSet[['MRI_ID','dem_labels']]\n",
    "\n",
    "\n",
    "# Link imageInfoDf_x dataframe (i.e. containing image metadata) to oasis1DataSet (i.e. containing labels) \n",
    "# to create `oasisMasterDf_x`\n",
    "\n",
    "oasisMasterDf_t = imageInfoDf_t.merge(right =oasis1DataSet,on='MRI_ID')\n",
    "oasisMasterDf_s = imageInfoDf_s.merge(right =oasis1DataSet,on='MRI_ID')\n",
    "oasisMasterDf_c = imageInfoDf_c.merge(right =oasis1DataSet,on='MRI_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4 : Create arrays and serialize to files using pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 - Create numpy `ndarray` of all the labels in the same order as it is in `oasisMasterDf`\n",
    "\n",
    "#### Also, optionally creating a list of the `MRI_ID`, in case this is necessary for any downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_processed_t = oasisMasterDf_t['dem_labels'].to_numpy()\n",
    "all_mri_id_processed_t = oasisMasterDf_t['MRI_ID'].to_numpy()\n",
    "\n",
    "all_labels_processed_s = oasisMasterDf_s['dem_labels'].to_numpy()\n",
    "all_mri_id_processed_s = oasisMasterDf_s['MRI_ID'].to_numpy()\n",
    "\n",
    "all_labels_processed_c = oasisMasterDf_c['dem_labels'].to_numpy()\n",
    "all_mri_id_processed_c = oasisMasterDf_c['MRI_ID'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 - Serialize and store each of `all_labels` and `all_mri_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/all_labels_processed_t.pickle\".format(baseSharedPath), \"wb\") as f:\n",
    "    pickle.dump(all_labels_processed_t, f)\n",
    "    \n",
    "with open(\"{}/all_mri_id_processed_t.pickle\".format(baseSharedPath), \"wb\") as f:\n",
    "    pickle.dump(all_mri_id_processed_t, f)\n",
    "    \n",
    "with open(\"{}/all_labels_processed_s.pickle\".format(baseSharedPath), \"wb\") as f:\n",
    "    pickle.dump(all_labels_processed_s, f)\n",
    "    \n",
    "with open(\"{}/all_mri_id_processed_s.pickle\".format(baseSharedPath), \"wb\") as f:\n",
    "    pickle.dump(all_mri_id_processed_s, f)\n",
    "\n",
    "with open(\"{}/all_labels_processed_c.pickle\".format(baseSharedPath), \"wb\") as f:\n",
    "    pickle.dump(all_labels_processed_c, f)\n",
    "    \n",
    "with open(\"{}/all_mri_id_processed_c.pickle\".format(baseSharedPath), \"wb\") as f:\n",
    "    pickle.dump(all_mri_id_processed_c, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6 : Create stitiched `Transverse` arrays using `skipFrames=120` on each end to reduce dimensionality to `256 -120 -120 = 16` frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 - Create numpy `ndarray` of all the images in the same order as it is in `oasisMasterDf`.\n",
    "#### Here, we are keeping only the middle 16 slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convert image path to numpy ndarray as this is the one we will be using.\n",
    "ImgPathAsNumpy_t = oasisMasterDf_t['Full path'].to_numpy()\n",
    "ImgPathAsNumpy_s = oasisMasterDf_s['Full path'].to_numpy()\n",
    "ImgPathAsNumpy_c = oasisMasterDf_c['Full path'].to_numpy()\n",
    "\n",
    "# Step 1: Create vectorized function\n",
    "vec_func_gif = np.vectorize(imgPathtoNp,otypes=[np.ndarray])\n",
    "\n",
    "# Step 2: Apply to each of the datasets\n",
    "processed_img_t = vec_func_gif(ImgPathAsNumpy_t)\n",
    "processed_img_s = vec_func_gif(ImgPathAsNumpy_s)\n",
    "processed_img_c = vec_func_gif(ImgPathAsNumpy_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/processed_img_t.pickle\".format(baseSharedPath), \"wb\") as f:\n",
    "    pickle.dump(processed_img_t, f)\n",
    "with open(\"{}/processed_img_s.pickle\".format(baseSharedPath), \"wb\") as f:\n",
    "    pickle.dump(processed_img_s, f)\n",
    "with open(\"{}/processed_img_c.pickle\".format(baseSharedPath), \"wb\") as f:\n",
    "    pickle.dump(processed_img_c, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
