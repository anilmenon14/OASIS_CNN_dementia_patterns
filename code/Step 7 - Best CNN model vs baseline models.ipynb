{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-req:** Confirm that you are actually using a GPU instance by running `nvidia-smi` from terminal.  \n",
    "**Important Note:** You will not see GPU being usable unless those modules have been loaded when the session was created.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env variable to optimize memory usage\n",
    "import os\n",
    "import datetime\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "bffb94db78734e50a7edb02791d265de",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models,preprocessing,Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix,precision_score,recall_score,accuracy_score,log_loss,roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from IPython.display import display_html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path where files will be stored.\n",
    "# This is unpacked from the pickle file created in Step 0.\n",
    "\n",
    "with open('pickledHomeScratchShared.pickle', \"rb\") as f:\n",
    "    baseHomePath,baseScratchPath,baseSharedPath = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set memory growth on GPUs (another step for memory optimization)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "  except RuntimeError as e:\n",
    "    print(\"Error: \"+e)\n",
    "\n",
    "#Print GPU devices, if any, that are available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Physical GPU Devices: \", physical_devices)\n",
    "\n",
    "logical_devices = tf.config.list_logical_devices('GPU')\n",
    "print(\"Logical GPU Devices: \", logical_devices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 : Setup re-usuable functions to be able to train each of CNN, Naive Bayes, Random Forest and Dummy Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to help choose files based on file choice and view choice\n",
    "def chooseFiles(viewChoice,fileChoice):\n",
    "    if fileChoice == '608':\n",
    "        transverseFile = 'skip_120_stitched_imgs_t_all_608.pickle'\n",
    "        sagittalFile = 'skip_56_stitched_imgs_s_all_608.pickle'\n",
    "        coronalFile = 'skip_120_stitched_imgs_c_all_608.pickle'\n",
    "        labelsFile = 'all_labels_allImgs_608.pickle'\n",
    "        mriIDFile = 'all_mri_id_allImgs_608.pickle'\n",
    "    elif fileChoice == '809':\n",
    "        transverseFile = 'skip_120_stitched_imgs_t_all_809.pickle'\n",
    "        sagittalFile = 'skip_56_stitched_imgs_s_all_809.pickle'\n",
    "        coronalFile = 'skip_120_stitched_imgs_c_all_809.pickle'\n",
    "        labelsFile = 'all_labels_allImgs_809.pickle'\n",
    "        mriIDFile = 'all_mri_id_allImgs_809.pickle'   \n",
    "    elif fileChoice == '508':\n",
    "        transverseFile = 'skip_120_stitched_imgs_t.pickle'\n",
    "        sagittalFile = 'skip_56_stitched_imgs_s.pickle'\n",
    "        coronalFile = 'skip_120_stitched_imgs_c.pickle'\n",
    "        labelsFile = 'all_labels.pickle'\n",
    "        mriIDFile = 'all_mri_id.pickle'   \n",
    "    elif fileChoice in ['Processed','Pre-trained','Standard-ML-1D']:\n",
    "        transverseFile = 'processed_img_t.pickle'\n",
    "        sagittalFile = 'processed_img_s.pickle'\n",
    "        coronalFile = 'processed_img_c.pickle'\n",
    "        if viewChoice == 'Transverse':\n",
    "            labelsFile = 'all_labels_processed_t.pickle'\n",
    "            mriIDFile = 'all_mri_id_processed_t.pickle'\n",
    "        elif viewChoice == 'Sagittal':\n",
    "            labelsFile = 'all_labels_processed_s.pickle'\n",
    "            mriIDFile = 'all_mri_id_processed_s.pickle'\n",
    "        elif viewChoice == 'Coronal':\n",
    "            labelsFile = 'all_labels_processed_c.pickle'\n",
    "            mriIDFile = 'all_mri_id_processed_c.pickle'  \n",
    "    # Load objects from serialized files\n",
    "    if viewChoice == 'Transverse':\n",
    "        with open(\"{}/{}\".format(baseSharedPath,transverseFile), \"rb\") as f:\n",
    "            img_16frames = pickle.load(f)\n",
    "    elif viewChoice == 'Coronal':\n",
    "        with open(\"{}/{}\".format(baseSharedPath,coronalFile), \"rb\") as f:\n",
    "            img_16frames = pickle.load(f)\n",
    "    elif viewChoice == 'Sagittal':\n",
    "        with open(\"{}/{}\".format(baseSharedPath,sagittalFile), \"rb\") as f:\n",
    "            img_16frames = pickle.load(f)\n",
    "    with open(\"{}/{}\".format(baseSharedPath,labelsFile), \"rb\") as f:\n",
    "        all_labels = pickle.load(f)\n",
    "    with open(\"{}/{}\".format(baseSharedPath,mriIDFile), \"rb\") as f:\n",
    "        all_mri_id = pickle.load(f)\n",
    "        \n",
    "    # 'Stack' the images  since it currently is represented in form of a scalar's shape (x,). Needs to be (x,y,z)\n",
    "    img_16frames = np.stack(img_16frames, axis=0)\n",
    "\n",
    "    if fileChoice == 'Pre-trained': # InceptionV3 needs 3 channels, hence have to adjust\n",
    "        img_16frames = np.repeat(img_16frames[..., np.newaxis], 3, -1)\n",
    "        print('Adjusting image shape for Pre-trained model to {}'.format(img_16frames.shape))\n",
    "    elif fileChoice == 'Standard-ML-1D': # Flatten to 1-D for standard ML models E.g. Naive Bayes,Dummy Classifier,Random Forest\n",
    "        img_16frames = img_16frames.reshape(-1, img_16frames.shape[1]*img_16frames.shape[2])\n",
    "        print('Adjusting image shape for standard machine learning models to {}'.format(img_16frames.shape))\n",
    "    else: # Reshape the images to 4 dimensional (1st dim is number of images, 2nd is height, 3rd is width and 4th is scalar)\n",
    "        img_16frames = img_16frames.reshape(-1, img_16frames.shape[1], img_16frames.shape[2],1)\n",
    "\n",
    "    return(img_16frames,all_labels,all_mri_id)\n",
    "\n",
    "# Code to standardize dataset\n",
    "def standardizeImg(img_16frames):\n",
    "    # Flatten the array along the last dimension\n",
    "    img_16frames_flat = img_16frames.reshape(-1, img_16frames.shape[-1])\n",
    "    # Standardize the flattened array\n",
    "    scaler = StandardScaler()\n",
    "    img_16frames_flat_scaled = scaler.fit_transform(img_16frames_flat)\n",
    "    # Reshape the standardized array to its original shape and reassign to 'img_16frames'\n",
    "    img_16frames = img_16frames_flat_scaled.reshape(img_16frames.shape)\n",
    "    return img_16frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-usable code for generating best model using the hyperparamters found in sensitivity analysis\n",
    "def generateBestCoronalModel():\n",
    "    if viewChoice != 'Coronal':\n",
    "        raise ValueError(\"This function is not meant for model generation for view other than Coronal \")\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=128, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu'\n",
    "                    ,input_shape=(img_16frames.shape[1], img_16frames.shape[2], 1), name = \"C_2d_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu', name = \"C_2d_2\"))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu', name = \"C_2d_3\"))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu', name = \"C_2d_4\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(16, kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu', name = \"Dense_1\"))\n",
    "    model.add(layers.Dense(2))\n",
    "    tf.random.set_seed(seedValue)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy']\n",
    "                     )\n",
    "    return model\n",
    "\n",
    "def generateDummyClassifier():\n",
    "    model = DummyClassifier(strategy='prior',random_state=seedValue) \n",
    "    return model\n",
    "\n",
    "def generateNaiveBayes():\n",
    "    model = GaussianNB()\n",
    "    return model\n",
    "\n",
    "def generateRandomForest():\n",
    "    model = RandomForestClassifier(random_state=seedValue)\n",
    "    return model\n",
    "\n",
    "# Reusable code to generate f1_score, precision, recall and confusion matrix for a pair of X_test,y_test against a model\n",
    "def generateF1Score(model,X_test,y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    if fileChoice == 'Standard-ML-1D':\n",
    "        f1Score = f1_score(y_test, y_pred, average='macro')\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matx = pd.DataFrame(confusion_matrix(y_test, y_pred), index = ['Actual-Neg', 'Actual-Pos'], columns = ['Pred-Neg', 'Pred-Pos'])\n",
    "    else: # For 'Processed' , '809' etc.\n",
    "        y_pred = y_pred.round(1)\n",
    "        y_pred_binary = [0 if x[0] > x[1] else 1 for x in y_pred]\n",
    "        y_test_binary = list(y_test.reshape(1,-1)[0])\n",
    "        f1Score = f1_score(y_test_binary, y_pred_binary, average='macro')\n",
    "        precision = precision_score(y_test_binary, y_pred_binary)\n",
    "        recall = recall_score(y_test_binary, y_pred_binary)\n",
    "        accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "        conf_matx = pd.DataFrame(confusion_matrix(y_test_binary, y_pred_binary), index = ['Actual-Neg', 'Actual-Pos'], columns = ['Pred-Neg', 'Pred-Pos'])\n",
    "    return (f1Score,precision,recall,accuracy,conf_matx)\n",
    "\n",
    "def generateConfMatrix(CVScores,title=\" \"):\n",
    "    conf_mat_list = [modelCVScore[-1] for modelCVScore in CVScores]\n",
    "    i=0\n",
    "    for conf_matx in conf_mat_list:\n",
    "        i += 1\n",
    "        conf_matx_styler = conf_matx.style.set_table_attributes(\"style='display:inline'\").set_caption('{}--Iteration {}-{}'.format(title,i,'Coronal'))\n",
    "        display_html(conf_matx_styler._repr_html_(), raw=True)\n",
    "        \n",
    "def generateScoreTable(CVScores,title=\" \"):\n",
    "    modelDfC = pd.DataFrame([item[0:-1] for item in CVScores]\n",
    "            ,columns= [\"ActualNegativesInTrain\" ,\"ActualPositivesInTrain\"\n",
    "                              ,\"ActualNegativesInTest\", \"ActualPositivesInTest\"\n",
    "                        ,\"f1Score\",\"precision\",\"recall\",\"accuracy\"]\n",
    "            ,index=['Iteration {}'.format(i) for i in range(1,n_splits+1)])\n",
    "    modelDfC_styler = modelDfC.style.set_table_attributes(\"style='display:inline'\").set_caption('{}'.format(title)).set_table_styles([{\n",
    "                            'selector': 'caption',\n",
    "                            'props': [('font-size', '20px')]\n",
    "                        }])\n",
    "    display(modelDfC_styler)\n",
    "    return modelDfC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose seed value as 42 to be used\n",
    "seedValue = 42\n",
    "random.seed(seedValue) # Applies to whereever random library is used\n",
    "print('Seed value used throughout this notebook is {}'.format(seedValue))\n",
    "n_splits = 5\n",
    "print('Number of k-folds applied on each experiment is : {}'.format(seedValue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : Create best CNN model using the optimal values from Sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File choices\n",
    "# Best file to work with is 'Processed'\n",
    "fileChoice = 'Processed'\n",
    "print(\"Choice of file is : \",fileChoice)\n",
    "# Best view is deemed to be 'Coronal' as seen from 809 vs Processed vs Pre-trained\n",
    "viewChoice = 'Coronal'\n",
    "print(\"Choice of view is : \",viewChoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for CNN\n",
    "\n",
    "print('The choice of view is {}'.format(viewChoice))\n",
    "print('The choice of file is {}'.format(fileChoice))\n",
    "img_16frames,all_labels,all_mri_id = chooseFiles(viewChoice,fileChoice) # Generate dataset\n",
    "img_16frames = standardizeImg(img_16frames) # Standardize dataset\n",
    "CNNModelCVScores = [] # Empty list to store scores from each iteration\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "# Train for chosen view over k-folds using distributed strategy\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    # Iterate through each fold\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "        print('Iteration: {} for {}'.format(i+1,viewChoice))\n",
    "        # Generate model from scratch \n",
    "        model = generateBestCoronalModel()\n",
    "        checkpoint_filepath = \"{}/{}\".format(os.path.dirname(baseSharedPath),'tmp') # 'tmp' folder in shared space\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint( # Saves best model on each epoch, which can be loaded and used\n",
    "                filepath=checkpoint_filepath, save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "        ]\n",
    "        # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "        X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "        y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "        ActualNegativesInTrain ,ActualPositivesInTrain = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "        ActualNegativesInTest, ActualPositivesInTest= len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "        model.fit(X_train,  y_train, epochs=20, \n",
    "                        validation_data=(X_test, y_test),callbacks=callbacks,verbose=2)\n",
    "        history = model.load_weights(checkpoint_filepath) # Load best model that is available from ModelCheckpoint callback data\n",
    "        # Store the evaluation metrics for this fold\n",
    "        CNNModelCVScores.append([ActualNegativesInTrain ,ActualPositivesInTrain\n",
    "                              ,ActualNegativesInTest, ActualPositivesInTest\n",
    "                              ,*generateF1Score(model,X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"Convolutional Neural Network\"\n",
    "generateConfMatrix(CNNModelCVScores,title=\"{}\".format(modelName))\n",
    "modelDF_CNN_Coronal = generateScoreTable(CNNModelCVScores,title=\"5-Fold Validation results for {}\".format(modelName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : Create best NaiveBayes, DummyClassifier and RandomForest model using default hyperparameter values to be able to compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File choices\n",
    "# Best file to work with is 'Processed'\n",
    "fileChoice = 'Standard-ML-1D'\n",
    "print(\"Choice of file is : \",fileChoice)\n",
    "# Best view is deemed to be 'Coronal' as seen from 809 vs Processed vs Pre-trained\n",
    "viewChoice = 'Coronal'\n",
    "print(\"Choice of view is : \",viewChoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for Dummy Classifier\n",
    "\n",
    "print('The choice of view is {}'.format(viewChoice))\n",
    "print('The choice of file is {}'.format(fileChoice))\n",
    "img_16frames,all_labels,all_mri_id = chooseFiles(viewChoice,fileChoice) # Generate dataset\n",
    "img_16frames = standardizeImg(img_16frames) # Standardize dataset\n",
    "DummyModelCVScores = [] # Empty list to store scores from each iteration\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "# Train for chosen view over k-folds without using distributed strategy\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "#with mirrored_strategy.scope():\n",
    "\n",
    "# Iterate through each fold\n",
    "for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "    print('Iteration: {} for {}'.format(i+1,viewChoice))\n",
    "    # Generate model from scratch \n",
    "    model = generateDummyClassifier()\n",
    "    # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "    X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "    y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "    ActualNegativesInTrain ,ActualPositivesInTrain = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "    ActualNegativesInTest, ActualPositivesInTest= len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "    model.fit(X_train,  y_train)\n",
    "    # Store the evaluation metrics for this fold\n",
    "    DummyModelCVScores.append([ActualNegativesInTrain ,ActualPositivesInTrain\n",
    "                          ,ActualNegativesInTest, ActualPositivesInTest\n",
    "                          ,*generateF1Score(model,X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"Dummy Classifier\"\n",
    "generateConfMatrix(DummyModelCVScores,title=\"{}\".format(modelName))\n",
    "modelDF_Dummy_Coronal = generateScoreTable(DummyModelCVScores,title=\"5-Fold Validation results for {}\".format(modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for Naive Bayes\n",
    "\n",
    "print('The choice of view is {}'.format(viewChoice))\n",
    "print('The choice of file is {}'.format(fileChoice))\n",
    "img_16frames,all_labels,all_mri_id = chooseFiles(viewChoice,fileChoice) # Generate dataset\n",
    "img_16frames = standardizeImg(img_16frames) # Standardize dataset\n",
    "NaiveBayesModelCVScores = [] # Empty list to store scores from each iteration\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "# Train for chosen view over k-folds without using distributed strategy\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "#with mirrored_strategy.scope():\n",
    "\n",
    "# Iterate through each fold\n",
    "for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "    print('Iteration: {} for {}'.format(i+1,viewChoice))\n",
    "    # Generate model from scratch \n",
    "    model = generateNaiveBayes()\n",
    "    # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "    X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "    y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "    ActualNegativesInTrain ,ActualPositivesInTrain = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "    ActualNegativesInTest, ActualPositivesInTest= len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "    model.fit(X_train,  y_train)\n",
    "    # Store the evaluation metrics for this fold\n",
    "    NaiveBayesModelCVScores.append([ActualNegativesInTrain ,ActualPositivesInTrain\n",
    "                          ,ActualNegativesInTest, ActualPositivesInTest\n",
    "                          ,*generateF1Score(model,X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"Naive Bayes\"\n",
    "generateConfMatrix(NaiveBayesModelCVScores,title=\"{}\".format(modelName))\n",
    "modelDF_NaiveBayes_Coronal = generateScoreTable(NaiveBayesModelCVScores,title=\"5-Fold Validation results for {}\".format(modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for Naive Bayes\n",
    "\n",
    "print('The choice of view is {}'.format(viewChoice))\n",
    "print('The choice of file is {}'.format(fileChoice))\n",
    "img_16frames,all_labels,all_mri_id = chooseFiles(viewChoice,fileChoice) # Generate dataset\n",
    "img_16frames = standardizeImg(img_16frames) # Standardize dataset\n",
    "RandomForestModelCVScores = [] # Empty list to store scores from each iteration\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "# Train for chosen view over k-folds without using distributed strategy\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "#with mirrored_strategy.scope():\n",
    "\n",
    "# Iterate through each fold\n",
    "for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "    print('Iteration: {} for {}'.format(i+1,viewChoice))\n",
    "    # Generate model from scratch \n",
    "    model = generateRandomForest()\n",
    "    # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "    X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "    y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "    ActualNegativesInTrain ,ActualPositivesInTrain = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "    ActualNegativesInTest, ActualPositivesInTest= len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "    model.fit(X_train,  y_train)\n",
    "    # Store the evaluation metrics for this fold\n",
    "    RandomForestModelCVScores.append([ActualNegativesInTrain ,ActualPositivesInTrain\n",
    "                          ,ActualNegativesInTest, ActualPositivesInTest\n",
    "                          ,*generateF1Score(model,X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"Random Forest\"\n",
    "generateConfMatrix(RandomForestModelCVScores,title=\"{}\".format(modelName))\n",
    "modelDF_RandomForest_Coronal = generateScoreTable(RandomForestModelCVScores,title=\"5-Fold Validation results for {}\".format(modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([modelDF_CNN_Coronal.mean(),modelDF_Dummy_Coronal.mean()\n",
    " ,modelDF_NaiveBayes_Coronal.mean(),modelDF_RandomForest_Coronal.mean()]\n",
    "             ,index=['CNN','DummyClassifier','GaussianNaiveBayes','RandomForest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_vs_rest = {\n",
    "'CNN' : CNNModelCVScores,\n",
    "'DummyClassifier' : DummyModelCVScores,\n",
    "'GaussianNaiveBayes' : NaiveBayesModelCVScores,\n",
    "'RandomForest' : RandomForestModelCVScores\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle results with appropriate name so that it can be loaded and used if needed\n",
    "runDateTime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "with open(\"{}/cnn_vs_rest_{}fold_seed{}_{}.pickle\".format(baseSharedPath+'/model_scores',n_splits,seedValue,runDateTime), \"wb\") as f:\n",
    "    pickle.dump(cnn_vs_rest, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve for best model across different data sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here iterating over 50,100,150,200,250,300,350,400,436 samples to see effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File choices\n",
    "# Best file to work with is 'Processed'\n",
    "fileChoice = 'Processed'\n",
    "print(\"Choice of file is : \",fileChoice)\n",
    "# Best view is deemed to be 'Coronal' as seen from 809 vs Processed vs Pre-trained\n",
    "viewChoice = 'Coronal'\n",
    "print(\"Choice of view is : \",viewChoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for CNN\n",
    "\n",
    "print('The choice of view is {}'.format(viewChoice))\n",
    "print('The choice of file is {}'.format(fileChoice))\n",
    "\n",
    "CNNModelCVScores = [] # Empty list to store scores from each iteration\n",
    "for size in [50,100,150,200,250,300,350,400,436]:\n",
    "    img_16frames,all_labels,all_mri_id = chooseFiles(viewChoice,fileChoice) # Generate dataset\n",
    "    img_16frames = standardizeImg(img_16frames) # Standardize dataset\n",
    "    # Randomize order so that files chosen from truncating below does not have bias\n",
    "    indices = np.arange(img_16frames.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    img_16frames = img_16frames[indices]\n",
    "    all_labels = all_labels[indices]\n",
    "    # truncate file size here\n",
    "    img_16frames = img_16frames[0:size,:,:,:]\n",
    "    all_labels = all_labels[0:size]\n",
    "    # Train using StratifiedKFold on the truncated\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "    # Train for chosen view over k-folds using distributed strategy\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        # Iterate through each fold\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "            print('Iteration: {} for {}'.format(i+1,viewChoice))\n",
    "            # Generate model from scratch \n",
    "            model = generateBestCoronalModel()\n",
    "            checkpoint_filepath = \"{}/{}\".format(os.path.dirname(baseSharedPath),'tmp') # 'tmp' folder in shared space\n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint( # Saves best model on each epoch, which can be loaded and used\n",
    "                    filepath=checkpoint_filepath, save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "            ]\n",
    "            # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "            X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "            y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "            ActualNegativesInTrain ,ActualPositivesInTrain = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "            ActualNegativesInTest, ActualPositivesInTest= len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "            model.fit(X_train,  y_train, epochs=20, \n",
    "                            validation_data=(X_test, y_test),callbacks=callbacks,verbose=2)\n",
    "            history = model.load_weights(checkpoint_filepath) # Load best model that is available from ModelCheckpoint callback data\n",
    "            # Store the evaluation metrics for this fold\n",
    "            CNNModelCVScores.append([i+1,size,ActualNegativesInTrain ,ActualPositivesInTrain\n",
    "                          ,ActualNegativesInTest, ActualPositivesInTest,*generateF1Score(model,X_test,y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle results with appropriate name so that it can be loaded and used if needed\n",
    "runDateTime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "with open(\"{}/learning_curve_{}fold_seed{}_{}.pickle\".format(baseSharedPath+'/model_scores',n_splits,seedValue,runDateTime), \"wb\") as f:\n",
    "    pickle.dump(CNNModelCVScores, f)"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "4a566eafd5bf4566a0bc7e290534994f",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
