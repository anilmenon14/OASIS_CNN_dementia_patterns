{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74aab2f",
   "metadata": {},
   "source": [
    "# Extracting Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import GifImagePlugin\n",
    "from numpy import asarray\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, homogeneity_score, completeness_score\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path where files will be stored.\n",
    "# This is unpacked from the pickle file created in Step 0.\n",
    "\n",
    "with open('pickledHomeScratchShared.pickle', \"rb\") as f:\n",
    "    baseHomePath,baseScratchPath,baseSharedPath = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe of O1 and create labels off of CDR score\n",
    "df = pd.read_csv('{}/milestone_II_project/data/oasis_labelled_data/oasis_1_labelled_data.csv'.format(baseHomePath))\n",
    "df['CDR'] = df.CDR.fillna(0)\n",
    "df['demented'] = [1 if x > 0 else 0 for x in df.CDR]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize image arrays\n",
    "def standardizeImg(img_16frames):\n",
    "    # Flatten the array along the last dimension\n",
    "    img_16frames_flat = img_16frames.reshape(-1, img_16frames.shape[-1])\n",
    "    # Standardize the flattened array\n",
    "    scaler = StandardScaler()\n",
    "    img_16frames_flat_scaled = scaler.fit_transform(img_16frames_flat)\n",
    "    # Reshape the standardized array to its original shape and reassign to 'img_16frames'\n",
    "    img_16frames = img_16frames_flat_scaled.reshape(img_16frames.shape)\n",
    "    return img_16frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to  Display images\n",
    "# def showImg(ndarr):\n",
    "#     return plt.imshow(ndarr, cmap=plt.cm.gray_r, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a33f2",
   "metadata": {},
   "source": [
    "For local use of extracting array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408e29d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Get file list to loop through our function above\n",
    "files = list(df.ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0205ed9",
   "metadata": {},
   "source": [
    "### Load our Coronal pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coronalDict = 'processed_img_c_dict.pickle'\n",
    "\n",
    "with open(\"{}/{}\".format(baseSharedPath,coronalDict), \"rb\") as f:\n",
    "    arr_dict_c = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4cae8",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f4421",
   "metadata": {},
   "source": [
    "Recreate our Best CNN model using Coronal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20151739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(array):\n",
    "    # Keep y as a series so we have the idx of y_test when we split the dataset. \n",
    "    y = (df.demented)\n",
    "    X = [arr for pat, arr in list(array.items())]\n",
    "    X = [x.reshape(x.shape + (1,)) for x in X]\n",
    "    X = np.array(X)\n",
    "    X = standardizeImg(X)\n",
    "    # Split data to train and test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = seed, stratify = y)\n",
    "    # extract the file idx for y-test for visualization purposes down the line.\n",
    "    y_test_file_idx = y_test.index\n",
    "    y_test_file_idx\n",
    "    # convert y_train and y_test to be arrays to fit model\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = y_test.reshape(-1,1)\n",
    "    return x_train, x_test, y_train, y_test, y_test_file_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update hyper parameters\n",
    "def generate_model(view):\n",
    "    if view == 'Transverse':\n",
    "        rows = 208\n",
    "        cols = 176\n",
    "    elif view == 'Coronal':\n",
    "        rows = 176\n",
    "        cols = 176\n",
    "    elif view == 'Sagittal':\n",
    "        rows = 176\n",
    "        cols = 208\n",
    "    tf.random.set_seed(42)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=128, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu'\n",
    "                    ,input_shape=(rows, cols, 1), name = \"C_2d_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu', name = \"C_2d_2\"))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu', name = \"C_2d_3\"))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu', name = \"C_2d_4\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(16, kernel_regularizer = tf.keras.regularizers.L2(0.005), activation='relu', name = \"Dense_1\"))\n",
    "    model.add(layers.Dense(2))\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy']\n",
    "                     )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0eca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_scores(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = y_pred.round(1)\n",
    "    y_pred_binary = [0 if x[0] > x[1] else 1 for x in y_pred]\n",
    "    y_test_binary = list(y_test.reshape(1,-1)[0])\n",
    "    f1_S = f1_score(y_test_binary, y_pred_binary, average='macro')\n",
    "    \n",
    "    conf_matx = pd.DataFrame(confusion_matrix(y_test_binary, y_pred_binary), index = ['neg', 'pos'], columns = ['neg', 'pos'])\n",
    "    conf_matx.columns.name = 'Predicted'\n",
    "    conf_matx.index.name = 'Actual'\n",
    "    return f1_S, conf_matx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cec8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = generate_model('Coronal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_c, x_test_c, y_train_c, y_test_c, y_test_file_idx_c = split_train_test(arr_dict_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bfa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.fit(x_train_c,  \n",
    "            y_train_c, \n",
    "            epochs=11, \n",
    "            validation_data=(x_test_c, y_test_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.evaluate(x_test_c, y_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94aa2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_scores(model_c, x_test_c, y_test_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beca19f",
   "metadata": {},
   "source": [
    "# Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4ce71",
   "metadata": {},
   "source": [
    "First we will make a copy of our overall dataframe but we will only keep records of the files that are in our test set. We will record the cluster groups predicted by our algorithms on to this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d653dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset_df = df.iloc[y_test_file_idx_c].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd33c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the first dense layer as our CNN output so that we may extract the output as features to cluster with.\n",
    "layer_name = 'Dense_1'\n",
    "layer = model_c.get_layer(name=layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce49b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_model_c = tf.keras.models.Model(inputs = model_c.input, outputs =layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Features\n",
    "features = activation_model_c.predict(x_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the shape of the features as a sanity check. We have 109 patients and 16 features.\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b85a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our features are very dense, therefore we should not use pca\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f4a8e",
   "metadata": {},
   "source": [
    "### hierarchical clustering to find optimal clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731db89",
   "metadata": {},
   "source": [
    "The first clustering approach we will use is Agglomerative Clustering. Through this method, we do not need to specity the number of clusters we wish to generate. In addition, we can plot a tree diagram, or a dendrogram, to visually observe the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35353ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "        \n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('MRI Nodes')\n",
    "    plt.ylabel('Euclidean Distance')\n",
    "    dendrogram(linkage_matrix)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dendro_tree = AgglomerativeClustering(n_clusters=None, distance_threshold = 0, compute_distances= True, affinity= 'euclidean')\n",
    "dendro_tree.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3112ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(dendro_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084df87",
   "metadata": {},
   "source": [
    "Based off the dendogram, two looks the best. The distance between these two clusters is very large compared to distances within these clusters. We will use 2 clusters for our clustering analysis.\n",
    "\n",
    "While we are still using Agglomerative clustering, lets make a model that clusters our test set by 2 clusters and extract their labels. The features we extracted from the CNN will be our inputs for this model. Note: we do not apply PCA on these features as the data is sparse and PCA does not perform well with sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_scores(features, cluster_labels):\n",
    "    sil_score = silhouette_score(features, cluster_labels, random_state = 42)\n",
    "    cal_har_score = calinski_harabasz_score(features, cluster_labels)\n",
    "    dav_bou_score = davies_bouldin_score(features, cluster_labels)\n",
    "    return sil_score, cal_har_score, dav_bou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ea8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = np.zeros((4,4))\n",
    "scores = np.zeros((3,4))\n",
    "\n",
    "col_idx = 0\n",
    "for link in ['ward', 'complete', 'average', 'single']:\n",
    "    agg_clus = AgglomerativeClustering(n_clusters=2, distance_threshold = None, compute_distances= True, affinity= 'euclidean', linkage= link)\n",
    "    agg_clus.fit(features)\n",
    "    agg_labels = agg_clus.labels_\n",
    "    sil_score, cal_har_score, dav_bou_score = cluster_scores(features, agg_labels)\n",
    "    scores[:,col_idx] = round(sil_score,2), round(cal_har_score,2), round(dav_bou_score,2)\n",
    "    col_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_scores = pd.DataFrame(data= scores, columns= ['ward', 'complete', 'average', 'single'], index = ['silhouette', 'calinski_harabasz', 'davies_bouldin'])\n",
    "print(agg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf2d8c",
   "metadata": {},
   "source": [
    "Complete seems to do the best for silhouette and calinski harabasz. We will use 'complete' for visualization purposes and for ground truth evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac647dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_clus = AgglomerativeClustering(n_clusters=2, distance_threshold = None, compute_distances= True, affinity= 'euclidean', linkage= 'complete')\n",
    "agg_clus.fit(features)\n",
    "agg_labels = agg_clus.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make a color map for these labels for visualization purposes down the line.\n",
    "agg_cmap = ['orange' if x == 0 else 'green' for x in agg_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf917f",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d4d8d",
   "metadata": {},
   "source": [
    "Next we will turn our heads at a very basic clustering algorithm, K-Means. We will do the same as the Algomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61902222",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 2, random_state=42, max_iter = 300, algorithm= 'auto')\n",
    "kmeans.fit(features)\n",
    "k_means_labels = kmeans.labels_\n",
    "k_means_cmap = ['blue' if l == 0 else 'green' if l == 1 else 'red' for l in k_means_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f6e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "def get_three_center_nodes(features):\n",
    "    points = features.copy()\n",
    "    closest_nodes_c1 = []\n",
    "    closest_nodes_c2 = []\n",
    "    for x in range(3):\n",
    "        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, points)\n",
    "        closest_nodes_c1.append(closest[0])\n",
    "        closest_nodes_c2.append(closest[1])\n",
    "        for idx in closest:\n",
    "            points = np.delete(points, obj= idx, axis=0)\n",
    "    \n",
    "    c1_files = list(df.iloc[closest_nodes_c1].ID)\n",
    "    c2_files = list(df.iloc[closest_nodes_c2].ID)\n",
    "    return c1_files, c2_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b146dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_c1, closest_c2 = get_three_center_nodes(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax1 = fig.add_subplot(2,3,1)\n",
    "ax1.imshow(arr_dict_c[closest_c1[0]])\n",
    "# ax1.set_xticks([])\n",
    "# ax1.set_yticks([])\n",
    "ax2 = fig.add_subplot(2,3,2)\n",
    "ax2.imshow(arr_dict_c[closest_c1[1]])\n",
    "ax3 = fig.add_subplot(2,3,3)\n",
    "ax3.imshow(arr_dict_c[closest_c1[2]])\n",
    "ax4 = fig.add_subplot(2,3,4)\n",
    "ax4.imshow(arr_dict_c[closest_c2[0]])\n",
    "ax5 = fig.add_subplot(2,3,5)\n",
    "ax5.imshow(arr_dict_c[closest_c2[1]])\n",
    "ax6 = fig.add_subplot(2,3,6)\n",
    "ax6.imshow(arr_dict_c[closest_c2[2]])\n",
    "\n",
    "for ax in [ax1,ax2,ax3,ax4,ax5,ax6]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "ax1.set_ylabel('Cluster 1', size= 20)\n",
    "ax4.set_ylabel('Cluster 2', size = 20)\n",
    "ax1.set_title('1st closest to centroid')\n",
    "ax2.set_title('2nd closest to centroid')\n",
    "ax3.set_title('3rd closest to centroid')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad7322",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781b937",
   "metadata": {},
   "source": [
    "Our last clustering algorithm will be DBSCAN, a deterministic method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs_params = np.zeros((8,11))\n",
    "col_idx = 0\n",
    "row_idx = 0\n",
    "for eps in np.linspace(0.5,1.5,11):\n",
    "    for samps in np.linspace(15,22,8).astype(int):\n",
    "        dbs = DBSCAN(eps=eps, min_samples= samps) #eps= 3, min_samples= 7\n",
    "        dbs.fit(features)\n",
    "        dbs_labels = dbs.labels_\n",
    "        clusters = list(np.unique(dbs_labels))\n",
    "        if -1 in clusters:\n",
    "            clusters.remove(-1) # remove outliers\n",
    "        dbs_params[row_idx,col_idx] = len(clusters)\n",
    "        row_idx += 1\n",
    "    col_idx += 1\n",
    "    row_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b95955",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs_hyp_params = pd.DataFrame(data = dbs_params, columns = np.linspace(0.5,1.5,11), index = np.linspace(15,22,8).astype(int))\n",
    "dbs_hyp_params.index.rename('samples', inplace= True)\n",
    "dbs_hyp_params.columns.rename('eps', inplace= True)\n",
    "dbs_hyp_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa70956",
   "metadata": {},
   "source": [
    "We will use an eps of 1.1 and test sample values 17 to 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros((3,5))\n",
    "\n",
    "col_idx = 0\n",
    "for samp in range(17,22):\n",
    "    dbs = DBSCAN(eps=1.1, min_samples= samp) #eps= 3, min_samples= 7\n",
    "    dbs.fit(features)\n",
    "    dbs_labels = dbs.labels_\n",
    "    sil_score, cal_har_score, dav_bou_score = cluster_scores(features, dbs_labels)\n",
    "    scores[:,col_idx] = round(sil_score,2), round(cal_har_score,2), round(dav_bou_score,2)\n",
    "    col_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0b7e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbs_scores = pd.DataFrame(data= scores, columns= range(17,22), index = ['silhouette', 'calinski_harabasz', 'davies_bouldin'])\n",
    "\n",
    "dbs_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfafd9b",
   "metadata": {},
   "source": [
    "17 samples appear to be best. Lets train a DBSCAN model that uses eps = 1.1 and samples = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = DBSCAN(eps=1.1, min_samples= 17) #eps= 3, min_samples= 7\n",
    "dbs.fit(features)\n",
    "dbs_labels = dbs.labels_\n",
    "dbs_cmap =['red' if x ==0 else 'green' if x==1 else 'blue' if x==2 else 'black' for x in dbs_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e10695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbs_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3268c",
   "metadata": {},
   "source": [
    "### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41eaf4",
   "metadata": {},
   "source": [
    "Since we compute 2 clusters, we need to make sense of what each of those clusters represent. It may be easy for us to think that our clustering models will capture demented and non-demented groups; however, we need to consider other possibilities for our cluster differences. Besides the grouping of demented and non-demented, the clustering algorithms may group patients by gender or perhaps age groups.\n",
    "\n",
    "We will compute two binary variables, male and elder. Male will be 1 if the patient is a Male, 0 otherwise. Elder will be 1 if the patient is above the age of 60, 0 otherwise. When clustering, we need to consider how the algorithms cluster these patients. Besides the grouping of demented and not demented, the algorithm may cluster by gender or perhaps age groups. These will serve as our ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 2 ground truth labels\n",
    "testset_df['male'] = [1 if x == 'M' else 0 for x in testset_df['M/F']]\n",
    "testset_df['elder'] = [1 if x >=60 else 0 for x in testset_df.Age]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e951bbd",
   "metadata": {},
   "source": [
    "We will also import our clustering labels for our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8aea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df['k_mean_clus'] = k_means_labels\n",
    "testset_df['dbs_clus'] = dbs_labels\n",
    "testset_df['agg_clus'] = agg_labels\n",
    "testset_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d783f5",
   "metadata": {},
   "source": [
    "Lets create a correlation coeficient table for our cluster labels and our ground truth labels. We will then visualize a heatmap to see how our cluster labels correlates with each ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = testset_df[['demented', 'male', 'elder', 'agg_clus', 'k_mean_clus','dbs_clus']].corr().abs()\n",
    "corr = corr[:3][['agg_clus', 'k_mean_clus','dbs_clus']]\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (10,10))\n",
    "plt.matshow(corr, fignum=f.number, cmap =plt.cm.Blues)\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, size = 20)\n",
    "plt.yticks(range(len(corr.index)), corr.index, size= 20)\n",
    "plt.title('Cluster and Variable Correlation', size = 30, pad = 20)\n",
    "cb = plt.colorbar(shrink = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3459490",
   "metadata": {},
   "source": [
    "It appears that most of the clustering algorithms seem to have highest correlation with patient's age (elder). The next variable with the second highest correlation is demenia. Lastly, our clustering labels does not seem to correlate much to gender groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65028663",
   "metadata": {},
   "source": [
    "# Manifold Learning t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd41a8",
   "metadata": {},
   "source": [
    "Lets visualize our data and clustering in a two dimensional space. We have 16 features generated from our CNN and we can not really make any sense of each of these features due to the nature of neural networks. We can however, visualize the features in 2-dimensions by using t-SNE. \n",
    "\n",
    "Once mapped onto a 2-dimensional space, we can color code our datapoints through cluster labels and ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3921b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components = 2, init = 'random', perplexity= 10, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c622d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_2d = tsne.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6629b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_x, tsne_y = tsne_2d.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "demented_cmap = ['blue' if x == 0 else 'red' for x in y_test_c.reshape(1,-1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_x, tsne_y, c = demented_cmap);\n",
    "plt.xlabel('First t-SNE feature');\n",
    "plt.ylabel('Second t-SNE feature');\n",
    "plt.xlabel(None);\n",
    "plt.ylabel(None);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "plt.title('Demented Labels mapped on t-SNE', size = 14);\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='r', lw=0 ,label='demented',\n",
    "                          markerfacecolor='r', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='b', lw=0 ,label='Non-demented',\n",
    "                          markerfacecolor='b', markersize=10)]\n",
    "plt.legend(handles=legend_elements);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2904ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_cmap = ['blue' if x ==1 else 'red' for x in testset_df.male]\n",
    "plt.scatter(tsne_x, tsne_y, c = gender_cmap)\n",
    "plt.xlabel('First t-SNE feature');\n",
    "plt.ylabel('Second t-SNE feature');\n",
    "plt.xlabel(None);\n",
    "plt.ylabel(None);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "plt.title('Gender Labels mapped on t-SNE', size= 14)\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='r', lw=0 ,label='female',\n",
    "                          markerfacecolor='r', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='b', lw=0 ,label='male',\n",
    "                          markerfacecolor='b', markersize=10)]\n",
    "plt.legend(handles=legend_elements);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a942580",
   "metadata": {},
   "outputs": [],
   "source": [
    "elder_cmap = ['red' if x ==1 else 'blue' for x in testset_df.elder]\n",
    "plt.scatter(tsne_x, tsne_y, c = elder_cmap)\n",
    "plt.xlabel('First t-SNE feature');\n",
    "plt.ylabel('Second t-SNE feature');\n",
    "plt.xlabel(None);\n",
    "plt.ylabel(None);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "plt.title('Elder Labels mapped on t-SNE', size = 14);\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='r', lw=0 ,label='elder(age >= 60)',\n",
    "                          markerfacecolor='r', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='b', lw=0 ,label='non-elder(age < 60)',\n",
    "                          markerfacecolor='b', markersize=10)]\n",
    "plt.legend(handles=legend_elements);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_x, tsne_y, c = dbs_cmap)\n",
    "plt.xlabel('First t-SNE feature');\n",
    "plt.ylabel('Second t-SNE feature');\n",
    "plt.xlabel(None);\n",
    "plt.ylabel(None);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "plt.title('DBSCAN Cluster Labels mapped on t-SNE', size = 14);\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='g', lw=0 ,label='DBSCAN cluster 1',\n",
    "                          markerfacecolor='g', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='r', lw=0 ,label='DBSCAN cluster 2',\n",
    "                          markerfacecolor='r', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='k', lw=0 ,label='Outliers',\n",
    "                          markerfacecolor='k', markersize=10)]\n",
    "plt.legend(handles=legend_elements);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_x, tsne_y, c = k_means_cmap)\n",
    "plt.xlabel('First t-SNE feature');\n",
    "plt.ylabel('Second t-SNE feature');\n",
    "plt.xlabel(None);\n",
    "plt.ylabel(None);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "plt.title('K-Means Cluster Labels mapped on t-SNE', size = 14);\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='b', lw=0 ,label='K-Means cluster 1',\n",
    "                          markerfacecolor='b', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='g', lw=0 ,label='K-Means cluster 2',\n",
    "                          markerfacecolor='g', markersize=10)]\n",
    "plt.legend(handles=legend_elements);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb59640",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_x, tsne_y, c = agg_cmap)\n",
    "plt.xlabel('First t-SNE feature');\n",
    "plt.ylabel('Second t-SNE feature');\n",
    "plt.xlabel(None);\n",
    "plt.ylabel(None);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "plt.title('Agglomerative Cluster Labels mapped on t-SNE', size = 14);\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='orange', lw=0 ,label='Agglomerative cluster 1',\n",
    "                          markerfacecolor='orange', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='g', lw=0 ,label='Agglomerative cluster 2',\n",
    "                          markerfacecolor='g', markersize=10)]\n",
    "plt.legend(handles=legend_elements);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c3c5ec",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4cfcf",
   "metadata": {},
   "source": [
    "By looking at the t-SNE charts, we can see that our clustering models cluster our age groups pretty well. In addition, our clusters seems to capture a group of patients that are not demented and another group of patients that are demented or non-demented. This could be due to the nature of the age groups since dementia is highly correlated with age. \n",
    "\n",
    "Lets evaluate our models through a series of metrics. Because we have ground truth labels for demented and age, we will use ground truth metrics as well (completeness score and homogenity score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette_score- near 1 for good, near -1 for bad\n",
    "# calinski_harabasz_score - higher the score the better\n",
    "# davies_vouldin_score - lower the score the better\n",
    "# completeness and homogeneity scores- near 1 for good, near 0 for bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd89d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_eval(features, y_test, cluster_labels):\n",
    "    score_list = []\n",
    "    sil_score = silhouette_score(features, cluster_labels, random_state = 42)\n",
    "    cal_har_score = calinski_harabasz_score(features, cluster_labels)\n",
    "    dav_bou_score = davies_bouldin_score(features, cluster_labels)\n",
    "    complete_score = completeness_score(y_test, cluster_labels)\n",
    "    homogen_score = homogeneity_score(y_test, cluster_labels)\n",
    "#     print('silhouette_score: ', sil_score)\n",
    "#     print('calinski_harabasz_score: ', cal_har_score)\n",
    "#     print('davies_bouldin_score: ', dav_bou_score)\n",
    "#     print('completeness_score: ', complete_score)\n",
    "#     print('homogeneity_score: ', homogen_score)\n",
    "#     score_dict['silhouette_score'] = round(sil_score,2)\n",
    "#     score_dict['calinski_harabasz_score'] = round(cal_har_score,2)\n",
    "#     score_dict['davies_bouldin_score'] = round(dav_bou_score,2)\n",
    "#     score_dict['completeness_score'] = round(complete_score,2)\n",
    "#     score_dict['homogeneity_score'] = round(homogen_score,2)\n",
    "    score_list.append(round(sil_score,3))\n",
    "    score_list.append(round(cal_har_score,3))\n",
    "    score_list.append(round(dav_bou_score,3))\n",
    "    score_list.append(round(complete_score,3))\n",
    "    score_list.append(round(homogen_score,3))\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f874978a",
   "metadata": {},
   "source": [
    "Lets use demented and elder as ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['silhouette_score', 'calinski_harabasz_score', 'davies_bouldin_score', 'completeness_score', 'homogeneity_score']\n",
    "index = ['Agg_vs_Demented', 'Kmean_vs_Demented', 'DBS_vs_Demented', 'Agg_vs_Elder', 'Kmean_vs_Elder', 'DBS_vs_Elder']\n",
    "final_results_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = np.zeros((6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores[0,:] = unsupervised_eval(features, y_test_c.flatten(), agg_labels)\n",
    "final_scores[1,:] = unsupervised_eval(features, y_test_c.flatten(), k_means_labels)\n",
    "final_scores[2,:] = unsupervised_eval(features, y_test_c.flatten(), dbs_labels)\n",
    "final_scores[3,:] = unsupervised_eval(features, testset_df.elder, agg_labels)\n",
    "final_scores[4,:] = unsupervised_eval(features, testset_df.elder, k_means_labels)\n",
    "final_scores[5,:] = unsupervised_eval(features, testset_df.elder, dbs_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b81a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df = pd.DataFrame(data = final_scores.round(3), columns=columns, index = index)\n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85d13c",
   "metadata": {},
   "source": [
    "We can see that K-Means performs the best for metrics that do not require ground truth labels. For our ground truth metrics, we compared the clusters to demented groups and age groups. K-Means performs best for both labels with scores that are relatively great at capturing age groups and scores that are not so great at capturing demented groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb653fa",
   "metadata": {},
   "source": [
    "### K-Means Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690302bb",
   "metadata": {},
   "source": [
    "K-Means does not really have many tunable hyper-parameters. The only concern regarding K-Means would be a bad initial centroid points. Therefore, We will run 5 different random initial points and observe how it affects the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5161564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(features, y_test, cluster_labels):\n",
    "    score_list = []\n",
    "    sil_score = silhouette_score(features, cluster_labels, random_state = 42)\n",
    "    cal_har_score = calinski_harabasz_score(features, cluster_labels)\n",
    "    dav_bou_score = davies_bouldin_score(features, cluster_labels)\n",
    "    complete_score = completeness_score(y_test, cluster_labels)\n",
    "    homogen_score = homogeneity_score(y_test, cluster_labels)\n",
    "    print('silhouette_score: ', sil_score)\n",
    "    print('calinski_harabasz_score: ', cal_har_score)\n",
    "    print('davies_bouldin_score: ', dav_bou_score)\n",
    "    print('completeness_score: ', complete_score)\n",
    "    print('homogeneity_score: ', homogen_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c48562",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    seed = np.random.randint(1000)\n",
    "    print(\"seed number: \",seed)\n",
    "    kmeans = KMeans(n_clusters = 2, init= 'random', random_state=seed, max_iter = 300, algorithm= 'auto')\n",
    "    kmeans.fit(features)\n",
    "    k_means_labels = kmeans.labels_\n",
    "    display_scores(features, testset_df.elder, k_means_labels)\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4cc733",
   "metadata": {},
   "source": [
    "In addition, lets try 'elkan algroithm instead of 'lloyd' (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    seed = np.random.randint(1000)\n",
    "    print(\"seed number: \",seed)\n",
    "    kmeans = KMeans(n_clusters = 2, init= 'random', random_state=seed, max_iter = 300, algorithm= 'elkan')\n",
    "    kmeans.fit(features)\n",
    "    k_means_labels_elkan = kmeans.labels_\n",
    "    display_scores(features, testset_df.elder, k_means_labels_elkan)\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0ac25",
   "metadata": {},
   "source": [
    "Nothing really changes. Based on the structure of the data as visualized through t-SNE, the data seems to be structured as a chain. Regardless of where the initial points start, the results always end up as the same.\n",
    "\n",
    "What if we used pca to squash down the 16 features to two? How will that affect our K-mean model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddacb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(features.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1, pc2 = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c47c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 2, init= 'random', random_state=42, max_iter = 300, algorithm= 'elkan')\n",
    "kmeans.fit(pca.components_.transpose())\n",
    "k_means_labels_pca = kmeans.labels_\n",
    "display_scores(features, testset_df.elder, k_means_labels_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_analysis = np.zeros((3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_analysis[0,:] = unsupervised_eval(features, testset_df.elder, k_means_labels)\n",
    "sens_analysis[1,:] = unsupervised_eval(features, testset_df.elder, k_means_labels_elkan)\n",
    "sens_analysis[2,:] = unsupervised_eval(features, testset_df.elder, k_means_labels_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d2c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['Normal K-Means', 'K-Means with Elkan', 'K-Means with PCA']\n",
    "sens_analysis = pd.DataFrame(data = sens_analysis, columns = columns, index = index)\n",
    "sens_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7d7de",
   "metadata": {},
   "source": [
    "It does a bit worse in terms of using ground truth labels. Overall our K-Means model is very insensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab696da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
