{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "1. We use 'Coronal' plane to be able to get insight into sensitivity analysis.\n",
    "2. Sensitivity analysis using 'val_accuracy',  'val_recall' and  'val_precision' on the y-axis\n",
    "3. Choices of x-axis are:  \n",
    "    a) Filter kernel sizes in 1st layer - 3,5,7,9,11  \n",
    "    b) Number of filter sizes in 1st layer - 8,16,32,64,128,256 (show in log scale)  \n",
    "    c) Neurons in fully connected layer - 16,32,64,128,256  \n",
    "    d) Learning rate of model -  0.0001,0.001,0.01,0.1,1\n",
    "    e) Regularizer values applied in each layer - \n",
    "4. We ensure to use 5-fold runs for each one.\n",
    "4. Final results are pickled into 'sen_analysis' folder in shared space after reach run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env variable to optimize memory usage\n",
    "import os\n",
    "import datetime\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "bffb94db78734e50a7edb02791d265de",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models,preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix,precision_score,recall_score,accuracy_score,log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import display_html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path where files will be stored.\n",
    "# This is unpacked from the pickle file created in Step 0.\n",
    "\n",
    "with open('pickledHomeScratchShared.pickle', \"rb\") as f:\n",
    "    baseHomePath,baseScratchPath,baseSharedPath = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set memory growth on GPUs (another step for memory optimization)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "  except RuntimeError as e:\n",
    "    print(\"Error: \"+e)\n",
    "\n",
    "#Print GPU devices, if any, that are available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Physical GPU Devices: \", physical_devices)\n",
    "\n",
    "logical_devices = tf.config.list_logical_devices('GPU')\n",
    "print(\"Logical GPU Devices: \", logical_devices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice has been made to use `Coronal` view using `Processed` and proceed since it appears to be best performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to help choose files based on file choice and view choice\n",
    "seedValue = 42 # Used across notebook where seed value has to be specified\n",
    "n_splits = 5 # Used later for k-fold\n",
    "coronalFile = 'processed_img_c.pickle'\n",
    "labelsFile = 'all_labels_processed_c.pickle'\n",
    "mriIDFile = 'all_mri_id_processed_c.pickle'  \n",
    "with open(\"{}/{}\".format(baseSharedPath,coronalFile), \"rb\") as f:\n",
    "    img_16frames = pickle.load(f)\n",
    "with open(\"{}/{}\".format(baseSharedPath,labelsFile), \"rb\") as f:\n",
    "    all_labels = pickle.load(f)\n",
    "with open(\"{}/{}\".format(baseSharedPath,mriIDFile), \"rb\") as f:\n",
    "    all_mri_id = pickle.load(f)\n",
    "        \n",
    "# 'Stack' the images  since it currently is represented in form of a scalar's shape (x,). Needs to be (x,y,z)\n",
    "img_16frames = np.stack(img_16frames, axis=0)\n",
    "img_16frames = img_16frames.reshape(-1, img_16frames.shape[1], img_16frames.shape[2],1)\n",
    "\n",
    "    #return(img_16frames,all_labels,all_mri_id)\n",
    "\n",
    "# Code to standardize dataset\n",
    "def standardizeImg(img_16frames):\n",
    "    # Flatten the array along the last dimension\n",
    "    img_16frames_flat = img_16frames.reshape(-1, img_16frames.shape[-1])\n",
    "    # Standardize the flattened array\n",
    "    scaler = StandardScaler()\n",
    "    img_16frames_flat_scaled = scaler.fit_transform(img_16frames_flat)\n",
    "    # Reshape the standardized array to its original shape and reassign to 'img_16frames'\n",
    "    img_16frames = img_16frames_flat_scaled.reshape(img_16frames.shape)\n",
    "    return img_16frames\n",
    "\n",
    "# Standardize dataset\n",
    "img_16frames = standardizeImg(img_16frames) # Standardize dataset\n",
    "print(img_16frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "272110bcead64c0bbf70600f6e747bef",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# No longer needed since this is being done inside the KFold code below.\n",
    "# X_train, X_test, y_train, y_test  = train_test_split(img_16frames, all_labels\n",
    "# ,test_size=0.2, random_state=seedValue , stratify=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ff0083417088409cab6716385d40e67b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# Show sample image of one of the images based on the view that was picked\n",
    "\n",
    "def plot_stitched_img(stitched_img):\n",
    "    # takes arrays from get_mri_array function.\n",
    "    # returns a sample of the image.\n",
    "    plt.close();\n",
    "    plt.figure(figsize=(50,30)) \n",
    "    plt.imshow(stitched_img, cmap=plt.cm.gray_r, interpolation=\"nearest\") \n",
    "    plt.show()\n",
    "\n",
    "#plot_stitched_img(img_16frames[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define re-usuable code to be able to generate model over each of the cross validation folds for each hyper-parameter choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the base model over which iterations are actually being done\n",
    "def generateModelFilter():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=32, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu'\n",
    "                    ,input_shape=(img_16frames.shape[1], img_16frames.shape[2], 1), name = \"C_2d_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_2\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_3\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_4\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"Dense_1\"))\n",
    "    model.add(layers.Dense(2))\n",
    "    tf.random.set_seed(seedValue)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 1 - Re-usable code for generating model using filter kernel sizes in 1st layer\n",
    "def generateModelKernelSize(kernelSize):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=32, kernel_size= kernelSize\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu'\n",
    "                    ,input_shape=(img_16frames.shape[1], img_16frames.shape[2], 1), name = \"C_2d_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_2\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_3\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_4\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"Dense_1\"))\n",
    "    model.add(layers.Dense(2))\n",
    "    tf.random.set_seed(seedValue)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 2 - Re-usable code for generating model using number of filters in 1st layer\n",
    "def generateModelFilter(numFilters):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=numFilters, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu'\n",
    "                    ,input_shape=(img_16frames.shape[1], img_16frames.shape[2], 1), name = \"C_2d_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_2\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_3\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_4\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"Dense_1\"))\n",
    "    model.add(layers.Dense(2))\n",
    "    tf.random.set_seed(seedValue)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 3 - Re-usable code for generating model using number of dense neurons in final layer\n",
    "def generateModelDenseNeuron(denseNeuron):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=32, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu'\n",
    "                    ,input_shape=(img_16frames.shape[1], img_16frames.shape[2], 1), name = \"C_2d_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_2\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_3\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_4\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(denseNeuron, kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"Dense_1\"))\n",
    "    model.add(layers.Dense(2))\n",
    "    tf.random.set_seed(seedValue)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 4 - Re-usable code for generating model using learning rate of model\n",
    "def generateModelLearningRate(learnRate):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=32, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu'\n",
    "                    ,input_shape=(img_16frames.shape[1], img_16frames.shape[2], 1), name = \"C_2d_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_2\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_3\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_4\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"Dense_1\"))\n",
    "    model.add(layers.Dense(2))\n",
    "    tf.random.set_seed(seedValue)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=learnRate),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 5 - Re-usable code for generating model using l2 reg\n",
    "def generateModelL2Reg(l2Reg):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=32, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(l2Reg), activation='relu'\n",
    "                    ,input_shape=(img_16frames.shape[1], img_16frames.shape[2], 1), name = \"C_2d_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(l2Reg), activation='relu', name = \"C_2d_2\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(l2Reg), activation='relu', name = \"C_2d_3\"))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(l2Reg), activation='relu', name = \"C_2d_4\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, kernel_regularizer = tf.keras.regularizers.L2(l2Reg), activation='relu', name = \"Dense_1\"))\n",
    "    model.add(layers.Dense(2))\n",
    "    tf.random.set_seed(seedValue)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 5 - Re-usable code for generating model using drop out choices in both the drop out layers.\n",
    "def generateModelDropOut(dropOut):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=32, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu'\n",
    "                    ,input_shape=(img_16frames.shape[1], img_16frames.shape[2], 1), name = \"C_2d_1\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_2\"))\n",
    "    model.add(layers.Dropout(dropOut))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_3\"))\n",
    "    model.add(layers.Dropout(dropOut))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size= 3\n",
    "                    ,kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"C_2d_4\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, kernel_regularizer = tf.keras.regularizers.L2(0.01), activation='relu', name = \"Dense_1\"))\n",
    "    model.add(layers.Dense(2))\n",
    "    tf.random.set_seed(seedValue)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Reusable code to generate f1_score, precision, recall and confusion matrix for a pair of X_test,y_test against a model\n",
    "def generateValScores(model,X_test,y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = y_pred.round(1)\n",
    "    y_pred_binary = [0 if x[0] > x[1] else 1 for x in y_pred]\n",
    "    y_test_binary = list(y_test.reshape(1,-1)[0])\n",
    "    f1Score = f1_score(y_test_binary, y_pred_binary, average='macro')\n",
    "    precision = precision_score(y_test_binary, y_pred_binary)\n",
    "    recall = recall_score(y_test_binary, y_pred_binary)\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "    return (f1Score,precision,recall,accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity analysis 1 : Filter kernel sizes - 3,5,7,9,11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for KernelSizes\n",
    "kernelSizes = [3,5,7,9,11]\n",
    "kernelSenScores = [] # Empty list to store scores from each iteration\n",
    "for kernelSize in kernelSizes:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "    # Train for chosen view over k-folds using distributed strategy\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        # Iterate through each fold\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "            print('Iteration: {} for Coronal for kernelSize={}'.format(i+1,kernelSize))\n",
    "            # Generate model from scratch \n",
    "            model = generateModelKernelSize(kernelSize)\n",
    "            checkpoint_filepath = \"{}/{}\".format(os.path.dirname(baseSharedPath),'tmp') # 'tmp' folder in shared space\n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint( # Saves best model on each epoch, which can be loaded and used\n",
    "                    filepath=checkpoint_filepath, save_weights_only=True\n",
    "                    ,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "            ]\n",
    "            # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "            X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "            y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "            model.fit(X_train,  y_train, epochs=20, \n",
    "                            validation_data=(X_test, y_test),callbacks=callbacks,verbose=2)\n",
    "            history = model.load_weights(checkpoint_filepath) # Load best model that is available from ModelCheckpoint callback data\n",
    "            # Store the evaluation metrics for this fold\n",
    "            kernelSenScores.append([kernelSize,i+1,*generateValScores(model,X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle results with appropriate name so that it can be loaded and used if needed\n",
    "runDateTime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "with open(\"{}/kernelSenScores_{}fold_seed{}_{}.pickle\".format(baseSharedPath+'/sen_analysis',n_splits,seedValue,runDateTime), \"wb\") as f:\n",
    "    pickle.dump(kernelSenScores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity analysis 2 : Number of filters - 8,16,32,64,128,256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for number of filters\n",
    "numFiltersChoices = [8,16,32,64,128,256]\n",
    "numFiltersSenScores = [] # Empty list to store scores from each iteration\n",
    "for numFilters in numFiltersChoices:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "    # Train for chosen view over k-folds using distributed strategy\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        # Iterate through each fold\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "            print('Iteration: {} for Coronal for numFilters={}'.format(i+1,numFilters))\n",
    "            # Generate model from scratch \n",
    "            model = generateModelFilter(numFilters)\n",
    "            checkpoint_filepath = \"{}/{}\".format(os.path.dirname(baseSharedPath),'tmp') # 'tmp' folder in shared space\n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint( # Saves best model on each epoch, which can be loaded and used\n",
    "                    filepath=checkpoint_filepath, save_weights_only=True\n",
    "                    ,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "            ]\n",
    "            # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "            X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "            y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "            model.fit(X_train,  y_train, epochs=20, \n",
    "                            validation_data=(X_test, y_test),callbacks=callbacks,verbose=2)\n",
    "            history = model.load_weights(checkpoint_filepath) # Load best model that is available from ModelCheckpoint callback data\n",
    "            # Store the evaluation metrics for this fold\n",
    "            numFiltersSenScores.append([numFilters,i+1,*generateValScores(model,X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle results with appropriate name so that it can be loaded and used if needed\n",
    "runDateTime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "with open(\"{}/numFiltersSenScores_{}fold_seed{}_{}.pickle\".format(baseSharedPath+'/sen_analysis',n_splits,seedValue,runDateTime), \"wb\") as f:\n",
    "    pickle.dump(numFiltersSenScores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity analysis 3 : Neurons in fully connected layer - 16,32,64,128,256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for dense layer neurons\n",
    "denseNeuronChoices = [8,16,32,64,128,256]\n",
    "denseNeuronSenScores = [] # Empty list to store scores from each iteration\n",
    "for denseNeuron in denseNeuronChoices:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "    # Train for chosen view over k-folds using distributed strategy\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        # Iterate through each fold\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "            print('Iteration: {} for Coronal for denseNeuron={}'.format(i+1,denseNeuron))\n",
    "            # Generate model from scratch \n",
    "            model = generateModelDenseNeuron(denseNeuron)\n",
    "            checkpoint_filepath = \"{}/{}\".format(os.path.dirname(baseSharedPath),'tmp') # 'tmp' folder in shared space\n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint( # Saves best model on each epoch, which can be loaded and used\n",
    "                    filepath=checkpoint_filepath, save_weights_only=True\n",
    "                    ,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "            ]\n",
    "            # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "            X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "            y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "            model.fit(X_train,  y_train, epochs=20, \n",
    "                            validation_data=(X_test, y_test),callbacks=callbacks,verbose=2)\n",
    "            history = model.load_weights(checkpoint_filepath) # Load best model that is available from ModelCheckpoint callback data\n",
    "            # Store the evaluation metrics for this fold\n",
    "            denseNeuronSenScores.append([denseNeuron,i+1,*generateValScores(model,X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pickle results with appropriate name so that it can be loaded and used if needed\n",
    "runDateTime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "with open(\"{}/denseNeuronSenScores_{}fold_seed{}_{}.pickle\".format(baseSharedPath+'/sen_analysis',n_splits,seedValue,runDateTime), \"wb\") as f:\n",
    "    pickle.dump(denseNeuronSenScores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity analysis 4 : Learning rate of model - 0.0001,0.001,0.01,0.1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for learning rate\n",
    "learnRateChoices = [0.0001,0.001,0.01,0.1,1]\n",
    "learnRateSenScores = [] # Empty list to store scores from each iteration\n",
    "for learnRate in learnRateChoices:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "    # Train for chosen view over k-folds using distributed strategy\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        # Iterate through each fold\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "            print('Iteration: {} for Coronal for learnRate={}'.format(i+1,learnRate))\n",
    "            # Generate model from scratch \n",
    "            model = generateModelLearningRate(learnRate)\n",
    "            checkpoint_filepath = \"{}/{}\".format(os.path.dirname(baseSharedPath),'tmp') # 'tmp' folder in shared space\n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint( # Saves best model on each epoch, which can be loaded and used\n",
    "                    filepath=checkpoint_filepath, save_weights_only=True\n",
    "                    ,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "            ]\n",
    "            # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "            X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "            y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "            model.fit(X_train,  y_train, epochs=20, \n",
    "                            validation_data=(X_test, y_test),callbacks=callbacks,verbose=2)\n",
    "            history = model.load_weights(checkpoint_filepath) # Load best model that is available from ModelCheckpoint callback data\n",
    "            # Store the evaluation metrics for this fold\n",
    "            learnRateSenScores.append([learnRate,i+1,*generateValScores(model,X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle results with appropriate name so that it can be loaded and used if needed\n",
    "runDateTime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "with open(\"{}/learnRateSenScores_{}fold_seed{}_{}.pickle\".format(baseSharedPath+'/sen_analysis',n_splits,seedValue,runDateTime), \"wb\") as f:\n",
    "    pickle.dump(learnRateSenScores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis 5 - L2 regularizer choices that are applied several layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for l2 reg\n",
    "l2RegChoices = [0.001,0.005,0.01,0.05,0.1,0.2,0.5]\n",
    "l2RegSenScores = [] # Empty list to store scores from each iteration\n",
    "for l2Reg in l2RegChoices:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "    # Train for chosen view over k-folds using distributed strategy\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        # Iterate through each fold\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "            print('Iteration: {} for Coronal for l2Reg={}'.format(i+1,l2Reg))\n",
    "            # Generate model from scratch \n",
    "            model = generateModelL2Reg(l2Reg)\n",
    "            checkpoint_filepath = \"{}/{}\".format(os.path.dirname(baseSharedPath),'tmp') # 'tmp' folder in shared space\n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint( # Saves best model on each epoch, which can be loaded and used\n",
    "                    filepath=checkpoint_filepath, save_weights_only=True\n",
    "                    ,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "            ]\n",
    "            # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "            X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "            y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "            model.fit(X_train,  y_train, epochs=20, \n",
    "                            validation_data=(X_test, y_test),callbacks=callbacks,verbose=2)\n",
    "            history = model.load_weights(checkpoint_filepath) # Load best model that is available from ModelCheckpoint callback data\n",
    "            # Store the evaluation metrics for this fold\n",
    "            l2RegSenScores.append([l2Reg,i+1,*generateValScores(model,X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle results with appropriate name so that it can be loaded and used if needed\n",
    "runDateTime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "with open(\"{}/l2RegSenScores_{}fold_seed{}_{}.pickle\".format(baseSharedPath+'/sen_analysis',n_splits,seedValue,runDateTime), \"wb\") as f:\n",
    "    pickle.dump(l2RegSenScores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis 6 - Drop out fractions applied on the 2 drop out layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code cell for training over stratified k-fold data for drop out\n",
    "dropOutChoices = [0.05,0.1,0.2,0.3,0.4,0.5]\n",
    "dropOutSenScores = [] # Empty list to store scores from each iteration\n",
    "for dropOut in dropOutChoices:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seedValue) # Define the Stratified KFold object\n",
    "    # Train for chosen view over k-folds using distributed strategy\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        # Iterate through each fold\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(img_16frames,all_labels)):\n",
    "            print('Iteration: {} for Coronal for dropOut={}'.format(i+1,dropOut))\n",
    "            # Generate model from scratch \n",
    "            model = generateModelDropOut(dropOut)\n",
    "            checkpoint_filepath = \"{}/{}\".format(os.path.dirname(baseSharedPath),'tmp') # 'tmp' folder in shared space\n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint( # Saves best model on each epoch, which can be loaded and used\n",
    "                    filepath=checkpoint_filepath, save_weights_only=True\n",
    "                    ,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "            ]\n",
    "            # Generate new datasets for X_train, X_test, y_train, y_test on each run\n",
    "            X_train, X_test =  img_16frames[train_index],img_16frames[test_index]\n",
    "            y_train, y_test = all_labels[train_index],all_labels[test_index]\n",
    "            model.fit(X_train,  y_train, epochs=20, \n",
    "                            validation_data=(X_test, y_test),callbacks=callbacks,verbose=2)\n",
    "            history = model.load_weights(checkpoint_filepath) # Load best model that is available from ModelCheckpoint callback data\n",
    "            # Store the evaluation metrics for this fold\n",
    "            dropOutSenScores.append([dropOut,i+1,*generateValScores(model,X_test,y_test)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle results with appropriate name so that it can be loaded and used if needed\n",
    "runDateTime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "with open(\"{}/dropOutSenScores_{}fold_seed{}_{}.pickle\".format(baseSharedPath+'/sen_analysis',n_splits,seedValue,runDateTime), \"wb\") as f:\n",
    "    pickle.dump(dropOutSenScores, f)"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "4a566eafd5bf4566a0bc7e290534994f",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
