{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from IPython.display import display_html\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248795be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path where files will be stored.\n",
    "# This is unpacked from the pickle file created in Step 0.\n",
    "\n",
    "with open('pickledHomeScratchShared.pickle', \"rb\") as f:\n",
    "    baseHomePath,baseScratchPath,baseSharedPath = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500cb30",
   "metadata": {},
   "source": [
    "## Section 1 : Analysis between `all_models_scores...` pickle files in `model_scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43f5db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a list files to choose from.\n",
    "\n",
    "fileNameList = os.listdir(baseSharedPath+'/model_scores')\n",
    "fileNameList.sort()\n",
    "[print(fileName) for fileName in fileNameList if re.match('^all.+',fileName)];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07637f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'all_model_scores_Pre-trained_5fold_seed696_20230225_032515.pickle'\n",
    "n_splits = 5 # \n",
    "viewChoices = ['Transverse','Coronal','Sagittal']\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/model_scores',fileName), \"rb\") as f:\n",
    "    allModelsScores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f475907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate empty dict\n",
    "scoresDict = dict()\n",
    "\n",
    "scoresVarList = [\"scores_16-stitched_14\",\"scores_16-stitched_42\",\"scores_16-stitched_696\"\n",
    "                 ,\"scores_Pre-trained_14\",\"scores_Pre-trained_42\",\"scores_Pre-trained_696\"\n",
    "                 ,\"scores_Processed_14\",\"scores_Processed_42\",\"scores_Processed_696\"]\n",
    "\n",
    "# Load pickled file contents into the scoreDict\n",
    "for scores,fileName in zip(scoresVarList,fileNameList):\n",
    "    with open(\"{}/{}\".format(baseSharedPath+'/model_scores',fileName), \"rb\") as f:\n",
    "        scoresDict[scores] = pickle.load(f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to display all of the confusion matrices one over the other for \"scores_809_14\"\n",
    "\n",
    "conf_mat_list = [[allModelsScores[key][i][-1] for key in scoresDict[\"scores_16-stitched_14\"]] for i in range(n_splits)]\n",
    "i = 0\n",
    "for iteration in conf_mat_list:\n",
    "    i += 1\n",
    "    confT,confC,confS = iteration\n",
    "    confT_styler = confT.style.set_table_attributes(\"style='display:inline'\").set_caption('Iteration {}-{}'.format(i,'Transvere'))\n",
    "    confC_styler = confC.style.set_table_attributes(\"style='display:inline'\").set_caption('Iteration {}-{}'.format(i,'Coronal'))\n",
    "    confS_styler = confS.style.set_table_attributes(\"style='display:inline'\").set_caption('Iteration {}-{}'.format(i,'Sagittal'))\n",
    "    space = \"\\xa0\" * 10\n",
    "    display_html(confT_styler._repr_html_()+space+confC_styler._repr_html_()+space+confS_styler._repr_html_()\n",
    "             , raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack all of the models from \"scores_16-stitched_14\" and display\n",
    "print('Below are results for a {} K-Fold validation training'.format(n_splits))\n",
    "modelDfT,modelDfC,modelDfS = [pd.DataFrame([item[0:-1] for item in scoresDict[\"scores_16-stitched_14\"][key]]\n",
    "             ,columns= [\"ActualNegativesInTrain\" ,\"ActualPositivesInTrain\"\n",
    "                              ,\"ActualNegativesInTest\", \"ActualPositivesInTest\"\n",
    "                        ,\"f1Score\",\"precision\",\"recall\",\"accuracy\"]) for key in allModelsScores]\n",
    "\n",
    "modelDfT_styler = modelDfT.style.set_table_attributes(\"style='display:inline'\").set_caption('Transvere')\n",
    "modelDfC_styler = modelDfC.style.set_table_attributes(\"style='display:inline'\").set_caption('Coronal')\n",
    "modelDfS_styler = modelDfS.style.set_table_attributes(\"style='display:inline'\").set_caption('Sagittal')\n",
    "\n",
    "space = \"\\xa0\" * 10\n",
    "display_html(modelDfT_styler._repr_html_()+space+modelDfC_styler._repr_html_()+space+modelDfS_styler._repr_html_()\n",
    "             , raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156306ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Master dataframe that has all the scores from all of the iterations\n",
    "masterDf = None\n",
    "for dictKey in scoresDict:\n",
    "    modelDfT,modelDfC,modelDfS = [pd.DataFrame([item[0:-1] for item in scoresDict[dictKey][key]]\n",
    "             ,columns= [\"ActualNegativesInTrain\" ,\"ActualPositivesInTrain\"\n",
    "                              ,\"ActualNegativesInTest\", \"ActualPositivesInTest\"\n",
    "                        ,\"f1Score\",\"precision\",\"recall\",\"accuracy\"]\n",
    "            ,index=[1,2,3,4,5]) for key in scoresDict[dictKey]]\n",
    "    _,dataMode,seedNum = dictKey.split(\"_\")\n",
    "    modelDfT['plane'] = 'Transverse'\n",
    "    modelDfC['plane'] = 'Coronal'\n",
    "    modelDfS['plane'] = 'Sagittal'\n",
    "    concatenated = pd.concat([modelDfT, modelDfC, modelDfS])\n",
    "    concatenated.index = concatenated.index.set_names(['Iteration'])\n",
    "    concatenated.reset_index(inplace=True)\n",
    "    concatenated['dataMode'] = dataMode\n",
    "    concatenated['seedNum'] = seedNum\n",
    "    if masterDf is not None:\n",
    "        masterDf = pd.concat([masterDf,concatenated])\n",
    "    else:\n",
    "        masterDf = concatenated.copy()\n",
    "\n",
    "masterDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa668dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdDev = alt.Chart(masterDf).mark_errorbar(extent='stdev',color='darkorange').encode(\n",
    "        x=alt.X('recall:Q', axis=alt.Axis(tickMinStep=0.5, tickCount=20), title='Recall'),\n",
    "        y=alt.Y('plane:N', title='Plane')\n",
    "    )\n",
    "\n",
    "means = alt.Chart(masterDf).mark_circle(color='black').encode(\n",
    "        x=alt.X('mean(recall):Q', title='Recall'),\n",
    "        y=alt.Y('plane:N', title='Plane')\n",
    ")\n",
    "\n",
    "(stdDev+means).properties(\n",
    "    width = 200, height = 200\n",
    "    ).facet(\n",
    "    column = alt.Column('dataMode:N', header=alt.Header(title='Data representation used in model',titleFontSize=16)),\n",
    "    row= alt.Row('seedNum:N', header=alt.Header(title='Seed Number/Random State of experiment',titleFontSize=16))\n",
    ").resolve_axis(\n",
    "    x='independent',\n",
    "    y='independent',\n",
    ").configure_header(\n",
    "    labelFontSize=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e56705",
   "metadata": {},
   "source": [
    "## Section 2 : Sensitivity analysis of 6 parameters using pickle files in `sen_analysis`\n",
    "\n",
    "Based on choices of kernel sizes, number of filters and neurons in fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53618705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list files to choose from.\n",
    "# These files have been generated from Step 4.1\n",
    "senAnalysisFiles = os.listdir(baseSharedPath+'/sen_analysis')\n",
    "senAnalysisFiles.sort()\n",
    "[print(fileName) for fileName in senAnalysisFiles];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the scores back into memory from pickle files shown above\n",
    "fileName = 'kernelSenScores_5fold_seed42_20230225_050448.pickle'\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/sen_analysis',fileName), \"rb\") as f:\n",
    "    kernelSenScores = pickle.load(f)\n",
    "    \n",
    "fileName = 'numFiltersSenScores_5fold_seed42_20230225_051611.pickle'\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/sen_analysis',fileName), \"rb\") as f:\n",
    "    numFiltersSenScores = pickle.load(f)\n",
    "\n",
    "fileName = 'denseNeuronSenScores_5fold_seed42_20230225_052744.pickle'\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/sen_analysis',fileName), \"rb\") as f:\n",
    "    denseNeuronSenScores = pickle.load(f)\n",
    "    \n",
    "fileName = 'learnRateSenScores_5fold_seed42_20230225_054503.pickle'\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/sen_analysis',fileName), \"rb\") as f:\n",
    "    learnRateSenScores = pickle.load(f)\n",
    "\n",
    "fileName = 'l2RegSenScores_5fold_seed42_20230225_055811.pickle'\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/sen_analysis',fileName), \"rb\") as f:\n",
    "    l2RegSenScores = pickle.load(f)\n",
    "    \n",
    "fileName = 'dropOutSenScores_5fold_seed42_20230225_060637.pickle'\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/sen_analysis',fileName), \"rb\") as f:\n",
    "    dropOutSenScores = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate accChart,recallChart and precisionChart from the DataFrame with xVar and 3 yVar\n",
    "\n",
    "def generateChart(df,xVar,yVar1,yVar2,yVar3,accYScale=[0.82, 0.92]):\n",
    "    accLine = alt.Chart(df).mark_line(color='darkorange').encode(\n",
    "        x='{}:N'.format(xVar),\n",
    "        y = alt.Y('mean({})'.format(yVar1) ,scale=alt.Scale(domain=accYScale))\n",
    "    )\n",
    "    accBand = alt.Chart(df).mark_errorband(extent='ci',color='lightblue').encode(\n",
    "        x='{}:N'.format(xVar),\n",
    "        y=alt.Y('{}'.format(yVar1), title='Accuracy',scale=alt.Scale(domain=accYScale)),\n",
    "    )\n",
    "    recallLine = alt.Chart(df).mark_line(color='darkorange').encode(\n",
    "        x='{}:N'.format(xVar),\n",
    "        y = alt.Y('mean({})'.format(yVar2) ,scale=alt.Scale(domain=[0.0, 0.92]))\n",
    "    )\n",
    "    recallBand = alt.Chart(df).mark_errorband(extent='ci',color='lightblue').encode(\n",
    "        x='{}:N'.format(xVar),\n",
    "        y=alt.Y('{}'.format(yVar2), title='Recall',scale=alt.Scale(domain=[0.0, 0.92])),\n",
    "    )\n",
    "\n",
    "    precisionLine = alt.Chart(df).mark_line(color='darkorange').encode(\n",
    "        x='{}:N'.format(xVar),\n",
    "        y = alt.Y('mean({})'.format(yVar3) ,scale=alt.Scale(domain=[0.0, 0.92]))\n",
    "    )\n",
    "    precisionBand = alt.Chart(df).mark_errorband(extent='ci',color='lightblue').encode(\n",
    "        x='{}:N'.format(xVar),\n",
    "        y=alt.Y('{}'.format(yVar3), title='Precision',scale=alt.Scale(domain=[0.0, 0.92])),\n",
    "    )\n",
    "    \n",
    "    accChart = (accBand + accLine).properties(width = 250)\n",
    "    recallChart = (recallLine + recallBand).properties(width = 250)\n",
    "    precisionChart = (precisionLine + precisionBand).properties(width = 250)\n",
    "    \n",
    "    return (accChart,recallChart,precisionChart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f30d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - kernelSenScores DataFrame\n",
    "kernelSenScoresDf = pd.DataFrame(kernelSenScores,columns = ['Kernel_Size','Iteration',\"f1Score\",\"precision\",\"recall\",\"accuracy\"])\n",
    "\n",
    "accChartKernel, recallChartKernel, precisionChartKernel = generateChart(kernelSenScoresDf\n",
    "                                                                        ,'Kernel_Size','accuracy','recall','precision')\n",
    "\n",
    "(accChartKernel | recallChartKernel | precisionChartKernel).properties(\n",
    "title={\n",
    "        'text': 'Effect of Accuracy, Recall and Precision with increasing kernel size of 1st CNN layer from 3 through 11',\n",
    "        'subtitle' : ['This data has been created using 5-fold cross validation using the \"Coronal\" view on the \"Preprocessed dataset\"'\n",
    "                      ,'The blue space represents the confidence interval from gathering data over 5-fold using seed as 42.'\n",
    "                     ,'The orange line represents the mean value of the metric (i.e. Accuracy, recall and Precision respectively)'],\n",
    "        'fontSize':16,\n",
    "        'subtitleFontSize':12,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - numFiltersSenScores DataFrame\n",
    "numFiltersSenScoresDf = pd.DataFrame(numFiltersSenScores,columns = ['Num_Filters','Iteration',\"f1Score\",\"precision\",\"recall\",\"accuracy\"])\n",
    "\n",
    "accChartNumFilters, recallChartNumFilters, precisionChartNumFilters = generateChart(numFiltersSenScoresDf\n",
    "                                                                        ,'Num_Filters','accuracy','recall','precision')\n",
    "\n",
    "(accChartNumFilters | recallChartNumFilters | precisionChartNumFilters).properties(\n",
    "title={\n",
    "        'text': 'Effect of Accuracy, Recall and Precision with increasing number of filters of 1st CNN layer from 8 through 128',\n",
    "        'subtitle' : ['This data has been created using 5-fold cross validation using the \"Coronal\" view on the \"Preprocessed dataset\"'\n",
    "                      ,'The blue space represents the confidence interval from gathering data over 5-fold using seed as 42.'\n",
    "                     ,'The orange line represents the mean value of the metric (i.e. Accuracy, recall and Precision respectively)'],\n",
    "        'fontSize':16,\n",
    "        'subtitleFontSize':12,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fdfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - denseNeuronSenScores DataFrame\n",
    "denseNeuronSenScoresDf = pd.DataFrame(denseNeuronSenScores,columns = ['Dense_Neurons','Iteration',\"f1Score\",\"precision\",\"recall\",\"accuracy\"])\n",
    "\n",
    "accChartDenseNeurons, recallChartDenseNeurons, precisionChartDenseNeurons = generateChart(denseNeuronSenScoresDf\n",
    "                                                                        ,'Dense_Neurons','accuracy','recall','precision')\n",
    "\n",
    "(accChartDenseNeurons | recallChartDenseNeurons | precisionChartDenseNeurons).properties(\n",
    "title={\n",
    "        'text': 'Effect of Accuracy, Recall and Precision with increasing number of filters of first Dense layer from 8 through 128',\n",
    "        'subtitle' : ['This data has been created using 5-fold cross validation using the \"Coronal\" view on the \"Preprocessed dataset\"'\n",
    "                      ,'The blue space represents the confidence interval from gathering data over 5-fold using seed as 42.'\n",
    "                     ,'The orange line represents the mean value of the metric (i.e. Accuracy, recall and Precision respectively)'],\n",
    "        'fontSize':16,\n",
    "        'subtitleFontSize':12,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed599fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - learnRateSenScores DataFrame\n",
    "learnRateSenScoresDf = pd.DataFrame(learnRateSenScores,columns = ['Learn_Rate','Iteration',\"f1Score\",\"precision\",\"recall\",\"accuracy\"])\n",
    "\n",
    "accChartLearnRate, recallChartLearnRate, precisionChartLearnRate = generateChart(learnRateSenScoresDf\n",
    "                                                                        ,'Learn_Rate','accuracy','recall','precision'\n",
    "                                                                                ,accYScale=[0.7,1.0])\n",
    "\n",
    "(accChartLearnRate | recallChartLearnRate | precisionChartLearnRate).properties(\n",
    "title={\n",
    "        'text': 'Effect of Accuracy, Recall and Precision with increasing learning rate on \"Adam\" optimizer from 0.0001 through 1',\n",
    "        'subtitle' : ['This data has been created using 5-fold cross validation using the \"Coronal\" view on the \"Preprocessed dataset\"'\n",
    "                      ,'The blue space represents the confidence interval from gathering data over 5-fold using seed as 42.'\n",
    "                     ,'The orange line represents the mean value of the metric (i.e. Accuracy, recall and Precision respectively)'],\n",
    "        'fontSize':16,\n",
    "        'subtitleFontSize':12,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c139e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - l2RegSenScores DataFrame\n",
    "l2RegSenScoresDf = pd.DataFrame(l2RegSenScores,columns = ['L2_Regularizer','Iteration',\"f1Score\",\"precision\",\"recall\",\"accuracy\"])\n",
    "\n",
    "\n",
    "accChartL2Reg, recallChartL2Reg, precisionChartL2Reg = generateChart(l2RegSenScoresDf\n",
    "                                                                        ,'L2_Regularizer','accuracy','recall','precision'\n",
    "                                                                               ,accYScale=[0.7,1.0] ) \n",
    "\n",
    "(accChartL2Reg | recallChartL2Reg | precisionChartL2Reg).properties(\n",
    "title={\n",
    "        'text': 'Effect of Accuracy, Recall and Precision with increasing L2 regularization on applicable layers from 0.001 through 0.5',\n",
    "        'subtitle' : ['This data has been created using 5-fold cross validation using the \"Coronal\" view on the \"Preprocessed dataset\"'\n",
    "                      ,'The blue space represents the confidence interval from gathering data over 5-fold using seed as 42.'\n",
    "                     ,'The orange line represents the mean value of the metric (i.e. Accuracy, recall and Precision respectively)'],\n",
    "        'fontSize':16,\n",
    "        'subtitleFontSize':12,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - dropOutSenScores DataFrame\n",
    "dropOutSenScoresDf = pd.DataFrame(dropOutSenScores,columns = ['Drop_Out','Iteration',\"f1Score\",\"precision\",\"recall\",\"accuracy\"])\n",
    "\n",
    "\n",
    "accChartDropOut, recallChartDropOut, precisionChartDropOut = generateChart(dropOutSenScoresDf\n",
    "                                                                        ,'Drop_Out','accuracy','recall','precision'\n",
    "                                                                               ,accYScale=[0.7,1.0] ) \n",
    "\n",
    "(accChartDropOut | recallChartDropOut | precisionChartDropOut).properties(\n",
    "title={\n",
    "        'text': 'Effect of Accuracy, Recall and Precision with increasing Drop out fraction on applicable layers from 0.05 through 0.5',\n",
    "        'subtitle' : ['This data has been created using 5-fold cross validation using the \"Coronal\" view on the \"Preprocessed dataset\"'\n",
    "                      ,'The blue space represents the confidence interval from gathering data over 5-fold using seed as 42.'\n",
    "                     ,'The orange line represents the mean value of the metric (i.e. Accuracy, recall and Precision respectively)'],\n",
    "        'fontSize':16,\n",
    "        'subtitleFontSize':12,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f9c8d",
   "metadata": {},
   "source": [
    "## Section 3 : Analysis of `best_model...` pickle file in `model_scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list files to choose from.\n",
    "\n",
    "fileNameList = os.listdir(baseSharedPath+'/model_scores')\n",
    "fileNameList.sort()\n",
    "[print(fileName) for fileName in fileNameList if re.match('^best.+',fileName)];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'best_model_scores_Coronal_Processed_5fold_seed42_20230226_042907.pickle'\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/model_scores',fileName), \"rb\") as f:\n",
    "    modelCVScores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f67414",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_list = [modelCVScore[-1] for modelCVScore in modelCVScores]\n",
    "i=0\n",
    "for conf_matx in conf_mat_list:\n",
    "    i += 1\n",
    "    #confT,confC,confS = iteration\n",
    "    #confT_styler = confT.style.set_table_attributes(\"style='display:inline'\").set_caption('Iteration {}-{}'.format(i,'Transvere'))\n",
    "    conf_matx_styler = conf_matx.style.set_table_attributes(\"style='display:inline'\").set_caption('Iteration {}-{}'.format(i,'Coronal'))\n",
    "    #confS_styler = confS.style.set_table_attributes(\"style='display:inline'\").set_caption('Iteration {}-{}'.format(i,'Sagittal'))\n",
    "    space = \"\\xa0\" * 10\n",
    "    display_html(conf_matx_styler._repr_html_()\n",
    "             , raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scores from each of the 5 iterations for scores on cross validation\n",
    "\n",
    "modelDfC = pd.DataFrame([item[0:-1] for item in modelCVScores]\n",
    "            ,columns= [\"ActualNegativesInTrain\" ,\"ActualPositivesInTrain\"\n",
    "                              ,\"ActualNegativesInTest\", \"ActualPositivesInTest\"\n",
    "                        ,\"f1Score\",\"precision\",\"recall\",\"accuracy\"],index=['Iteration {}'.format(i) for i in range(1,6)])\n",
    "modelDfC_styler = modelDfC.style.set_table_attributes(\"style='display:inline'\").set_caption(\n",
    "    '5-fold cross validation results in each iteration for best \"Coronal\"+\"Processed\" model using seed=42').set_table_styles([{\n",
    "                            'selector': 'caption',\n",
    "                            'props': [('font-size', '20px')]\n",
    "                        }])\n",
    "display_html(modelDfC_styler._repr_html_()\n",
    "         , raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDfC.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebc6a2",
   "metadata": {},
   "source": [
    "## Section 4 : Compare each of the scores from CNN vs baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list files to choose from.\n",
    "\n",
    "fileNameList = os.listdir(baseSharedPath+'/model_scores')\n",
    "fileNameList.sort()\n",
    "[print(fileName) for fileName in fileNameList if re.match('^cnn.+',fileName)];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80896386",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'cnn_vs_rest_5fold_seed42_20230226_061528.pickle'\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/model_scores',fileName), \"rb\") as f:\n",
    "    cnnVsRest = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the values from dictionary and build into DataFrame to use with altair\n",
    "cnnVsRestList = []\n",
    "for modelType in cnnVsRest:\n",
    "    for result in cnnVsRest[modelType]:\n",
    "        cnnVsRestList.append([modelType,*result[4:-1]])\n",
    "cnnVsRestListDf = pd.DataFrame(cnnVsRestList,columns=[\"Model Type\",\"f1Score\",\"precision\",\"recall\",\"accuracy\"])\n",
    "meltedDf = pd.melt(cnnVsRestListDf, id_vars=[\"Model Type\"], var_name='Metric')\n",
    "meltedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2322cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce altair chart\n",
    "meanChart = alt.Chart(meltedDf).mark_bar().encode(\n",
    "    x='Metric:N',\n",
    "    y='mean(value):Q',\n",
    "    color='Model Type:N',\n",
    "    column='Model Type:N'\n",
    ").properties(width = 120)\n",
    "\n",
    "meanChart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance of results of CNN vs others\n",
    "\n",
    "listOfModels = ['DummyClassifier', 'GaussianNaiveBayes', 'RandomForest']\n",
    "listOfMetrics = list(meltedDf['Metric'].unique())\n",
    "compare_df = pd.DataFrame(columns=['Model Compared With', 'Metric', 't-statistic', 'p-value'])\n",
    "index = 0\n",
    "for metric in listOfMetrics:\n",
    "    for model in listOfModels:\n",
    "        CNNValue = meltedDf[(meltedDf['Metric'] == metric) & (meltedDf['Model Type'] == 'CNN')]['value']\n",
    "        CompareValue = meltedDf[(meltedDf['Metric'] == metric) & (meltedDf['Model Type'] == model)]['value']\n",
    "        result = stats.ttest_ind(CompareValue,CNNValue)\n",
    "        compare_df.loc[index,'Model Compared With'] = model\n",
    "        compare_df.loc[index,'Metric'] =  metric\n",
    "        compare_df.loc[index,'t-statistic'] = round(result.statistic,2)\n",
    "        compare_df.loc[index,'p-value'] = round(result.pvalue,2)\n",
    "        index += 1\n",
    "\n",
    "# Display compare_df\n",
    "compare_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5568d5c",
   "metadata": {},
   "source": [
    "## Section 5 : Learning curve with standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list files to choose from.\n",
    "\n",
    "fileNameList = os.listdir(baseSharedPath+'/model_scores')\n",
    "fileNameList.sort()\n",
    "[print(fileName) for fileName in fileNameList if re.match('^lear.+',fileName)];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'learning_curve_5fold_seed42_20230227_130500.pickle'\n",
    "with open(\"{}/{}\".format(baseSharedPath+'/model_scores',fileName), \"rb\") as f:\n",
    "    learnCurve = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "learnCurveDf = pd.DataFrame(learnCurve,columns=[\"Iteration\",\"Size\",\"ActualNegativesInTrain\" ,\"ActualPositivesInTrain\"\n",
    "                          ,\"ActualNegativesInTest\", \"ActualPositivesInTest\",\"f1Score\",\"precision\",\"recall\",\"accuracy\",\"conf_matrix\"])\n",
    "learnCurveDf.drop(columns=[\"ActualNegativesInTrain\" ,\"ActualPositivesInTrain\"\n",
    "                          ,\"ActualNegativesInTest\", \"ActualPositivesInTest\",\"conf_matrix\"],inplace=True)\n",
    "learnCurveDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdDev = alt.Chart(learnCurveDf).mark_errorbar(extent='stdev',color='darkorange').encode(\n",
    "        x=alt.X('Size:O',title=''),\n",
    "        y=alt.Y('recall:Q',title='')\n",
    "    )\n",
    "\n",
    "means = alt.Chart(learnCurveDf).mark_circle(color='black').encode(\n",
    "        x=alt.X('Size:O',title=''),\n",
    "        y=alt.Y('mean(recall):Q',title='')\n",
    ")\n",
    "\n",
    "line = alt.Chart(learnCurveDf).mark_line(color='blue').encode(\n",
    "        x=alt.X('Size:O', title='Size of total dataset (80% split to train and 20% to validate)'),\n",
    "        y=alt.Y('mean(recall):Q', title='Recall score on validation data')\n",
    ")\n",
    "\n",
    "\n",
    "(stdDev+means+line).properties(\n",
    "    width = 400, height = 300\n",
    "    ).properties(\n",
    "title={\n",
    "        'text': 'Effect of Training data size on recall of the model ',\n",
    "        'fontSize':16,\n",
    "        'subtitleFontSize':12,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
